{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WORKSTATION\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from tqdm import tqdm\n",
    "from time import time \n",
    "from utils import save_model\n",
    "from utils import DistilLog\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath = '..\\\\newdatasets'\n",
    "file_names = [f\"BGL_20l_train_{x}.pkl\" for x in [0.1, 0.2, 0.4, 0.6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../newdatasets/BGL_20l_train_0.1.pkl', 'rb') as file:\n",
    "    raw_data = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = raw_data[0] + raw_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235675"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.0003\n",
    "hidden_size = 128\n",
    "input_size = 300\n",
    "sequence_length = 20\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "save_teacher_path = '../newdatasets/bgl_20_teacher.pth'\n",
    "save_noKD_path = '../newdatasets/bgl_20_noKD.pth'\n",
    "\n",
    "Teacher = DistilLog(input_size, hidden_size, num_layers, num_classes, is_bidirectional=False).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in pbar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data)\n",
    "        target = target.long()\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            done = (batch_idx + 1) * len(data)\n",
    "            percentage = 100. * batch_idx / len(train_loader)\n",
    "            pbar.set_description(\n",
    "                f'Train Epoch: [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)]  Loss: {total_loss:.6f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "def load_data(train_x, train_y, batch_size):\n",
    "    #train_x = np.reshape(train_x, (train_x.shape[0], -1, input_size))\n",
    "    train_y = train_y.astype(int)\n",
    "    tensor_x = torch.Tensor(train_x)\n",
    "    tensor_y = torch.from_numpy(train_y)\n",
    "    train_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True)\n",
    "    return train_loader\n",
    "\n",
    "def process_data_in_chunks(data, chunk_size=20000):\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i:i + chunk_size]\n",
    "        log_vec_chunk = [np.array(item['Embeddings']) for item in chunk]\n",
    "        label = [np.array(item['Label']) for item in chunk]\n",
    "        yield log_vec_chunk, label\n",
    "\n",
    "for i in range (2):\n",
    "    print(\"epoch: \", i+1)    \n",
    "    for log, label in process_data_in_chunks(tmp_data):\n",
    "        log = np.array(log)\n",
    "        label = np.array(label)\n",
    "        data_loader = load_data(log, label, batch_size)\n",
    "        Teacher = train(Teacher, data_loader, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(Teacher, save_teacher_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../newdatasets/BGL_20l_train_0.8.pkl', 'rb') as file:\n",
    "    raw_test_data = pickle.load(file)\n",
    "\n",
    "test_data = raw_test_data[0] + raw_test_data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion = nn.CrossEntropyLoss()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        TP = 0 \n",
    "        FP = 0\n",
    "        FN = 0 \n",
    "        TN = 0\n",
    "\n",
    "        for log, label in process_data_in_chunks(test_data):\n",
    "            log = np.array(log)\n",
    "            label = np.array(label)\n",
    "            test_loader = load_data(log, label, batch_size)           \n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output, _ = model(data)\n",
    "                target = target.long()\n",
    "                test_loss += criterion(output, target) # sum up batch loss\n",
    "                \n",
    "                output = torch.sigmoid(output)[:, 0].cpu().detach().numpy()\n",
    "                predicted = (output < 0.2).astype(int)\n",
    "                target = np.array([y.cpu() for y in target])\n",
    "\n",
    "                TP += ((predicted == 1) * (target == 1)).sum()\n",
    "                FP += ((predicted == 1) * (target == 0)).sum()\n",
    "                FN += ((predicted == 0) * (target == 1)).sum()\n",
    "                TN += ((predicted == 0) * (target == 0)).sum()\n",
    "        P = 100 * TP / (TP + FP)\n",
    "        R = 100 * TP / (TP + FN)\n",
    "        F1 = 2 * P * R / (P + R)   \n",
    "        accuracy = 100 * (TP + TN)/(TP + TN + FP + FN)\n",
    "        #MCC = 100*(TP*TN + FP*FN)/math.sqrt((TP+FP)*(TN+FN)*(TN+FP)*(TP+FN))         \n",
    "    return accuracy, test_loss, P, R, F1, TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(Teacher, criterion = nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of testing teacher model\n",
      "false positive (FP): 0, false negative (FN): 233, true positive (TP): 101, true negative (TN): 1666\n",
      "Test set: Average loss: 40.8008, Accuracy: 88.35%).\n",
      "Precision: 100.000%, Recall: 30.240%, F1-measure: 46.437%\n"
     ]
    }
   ],
   "source": [
    "print('Result of testing teacher model')\n",
    "print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n",
    "print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%).')\n",
    "print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%' .format(P, R, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def train_step(\n",
    "    Teacher,\n",
    "    Student,\n",
    "    data_loader,\n",
    "    optimizer,\n",
    "    student_loss_fn,\n",
    "    divergence_loss_fn,\n",
    "    temp,\n",
    "    alpha,\n",
    "    epoch,\n",
    "    device\n",
    "):\n",
    "    losses = []\n",
    "    for data, targets in data_loader:\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            teacher_preds, _ = Teacher(data)\n",
    "\n",
    "        student_preds, __ = Student(data)\n",
    "        targets = targets.long()\n",
    "        student_loss = student_loss_fn(student_preds, targets)\n",
    "        \n",
    "        ditillation_loss = divergence_loss_fn(\n",
    "            F.softmax(student_preds / temp, dim=1),\n",
    "            F.softmax(teacher_preds / temp, dim=1)\n",
    "        )\n",
    "        loss = alpha * student_loss + (1 - alpha) * ditillation_loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teach(epochs, Teacher, Student, data_loader, temp, alpha):\n",
    "    Teacher = Teacher.to(device)\n",
    "    Student = Student.to(device)\n",
    "    student_loss_fn = nn.CrossEntropyLoss()\n",
    "    divergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    optimizer = torch.optim.Adam(Student.parameters(), lr=0.01)\n",
    "\n",
    "    Teacher.eval()\n",
    "    Student.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epochs: \", epoch+1)\n",
    "        loss = train_step(\n",
    "            Teacher,\n",
    "            Student,\n",
    "            data_loader,\n",
    "            optimizer,\n",
    "            student_loss_fn,\n",
    "            divergence_loss_fn,\n",
    "            temp,\n",
    "            alpha,\n",
    "            epoch,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(f\"Loss:{loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WORKSTATION\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "Student = DistilLog(input_size = input_size, hidden_size=4, num_layers = 1, num_classes = num_classes, is_bidirectional=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = full_data[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "Epochs:  1\n",
      "Loss:-0.77\n",
      "epoch:  2\n",
      "Epochs:  1\n",
      "Loss:-0.80\n"
     ]
    }
   ],
   "source": [
    "for i in range (2):\n",
    "    print(\"epoch: \", i+1)    \n",
    "    for log, label in process_data_in_chunks(tmp_data):\n",
    "        log = np.array(log)\n",
    "        label = np.array(label)\n",
    "        data_loader = load_data(log, label, batch_size)\n",
    "        teach(epochs=1, Teacher=Teacher, Student=Student, data_loader=data_loader, temp=7, alpha=0.3)\n",
    "\n",
    "#save_model(Student, save_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(Student, criterion = nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of testing student model\n",
      "false positive (FP): 8, false negative (FN): 454, true positive (TP): 8594, true negative (TN): 10944\n",
      "Test set: Average loss: 52.2439, Accuracy: 97.69%).\n",
      "Precision: 99.907%, Recall: 94.982%, F1-measure: 97.382%\n"
     ]
    }
   ],
   "source": [
    "print('Result of testing student model')\n",
    "print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n",
    "print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%).')\n",
    "print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%' .format(P, R, F1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
