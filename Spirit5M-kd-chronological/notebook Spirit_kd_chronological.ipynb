{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JcGRk9P3yeTW","outputId":"30595f03-0404-47ad-b67c-30645e65e6c0","executionInfo":{"status":"ok","timestamp":1665721326249,"user_tz":-420,"elapsed":19511,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_tOmaC1Dw3O","executionInfo":{"status":"ok","timestamp":1665721326720,"user_tz":-420,"elapsed":489,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"db3e2fc8-7a96-4c15-f817-62bbb1b54cec"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Oct 14 04:22:06 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0    44W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sMLgWD3s94U","outputId":"dbba047b-4b9b-4cff-a74c-bf30d6b8fb99","executionInfo":{"status":"ok","timestamp":1665721327992,"user_tz":-420,"elapsed":1276,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab/distillog/Spirit-kd-chronological\n"]}],"source":["%cd /content/drive/MyDrive/colab/distillog/Spirit-kd-chronological"]},{"cell_type":"code","source":["!pip install torchinfo\n","!pip install overrides"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvRaUhLQt2qH","outputId":"9f2832d7-c3cf-417d-e82d-3c8072f85a3c","executionInfo":{"status":"ok","timestamp":1665721401468,"user_tz":-420,"elapsed":6898,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting overrides\n","  Downloading overrides-7.1.0-py3-none-any.whl (17 kB)\n","Installing collected packages: overrides\n","Successfully installed overrides-7.1.0\n"]}]},{"cell_type":"code","source":["import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.nn import Parameter\n","from torch.nn.modules.module import Module\n","from tqdm import tqdm\n","import math\n","import csv\n","from time import time \n","from torchinfo import summary\n","from utils import save_model, train, read_data, load_data\n","from utils import DistilLog\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","num_classes = 2\n","batch_size = 50\n","learning_rate = 0.0005\n","hidden_size = 128\n","input_size = 30\n","sequence_length = 50\n","num_layers = 2\n","\n","train_path = '../datasets/Spirit/chronological_train.csv'\n","save_teacher_path = '../datasets/Spirit/model/chronological_teacher.pth'\n","save_noKD_path = '../datasets/Spirit/model/chronological_noKD.pth'\n","\n","Teacher = DistilLog(input_size, hidden_size, num_layers , num_classes, is_bidirectional=False).to(device)\n","noKD = DistilLog(input_size = input_size, hidden_size = 4, num_layers = 1, num_classes = num_classes, is_bidirectional=False).to(device)\n","#summary(Teacher, input_size=(50, 50, 30))\n","\n","train_x, train_y = read_data(train_path, input_size, sequence_length)\n","train_loader = load_data(train_x, train_y, batch_size)\n","\n","#Teacher = train(Teacher, train_loader, learning_rate = 0.0003, num_epochs = 400)\n","#save_model(Teacher, save_teacher_path)\n","\n"],"metadata":{"id":"D5TkzkbguHXA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae83b0ae-d3e3-4043-b5dc-03a3e2d78a37","executionInfo":{"status":"ok","timestamp":1665721745210,"user_tz":-420,"elapsed":16495,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","source":["Teacher = train(Teacher, train_loader, learning_rate = 0.0003, num_epochs = 200)\n","save_model(Teacher, save_teacher_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKACAbAzjFDf","outputId":"f17a8911-d39b-477a-a549-df58214171b5","executionInfo":{"status":"ok","timestamp":1665662796469,"user_tz":-420,"elapsed":1400243,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Train Epoch: 1/200 [78400/79999 (100%)]  Loss: 425.424866: 100%|██████████| 1600/1600 [00:07<00:00, 207.28it/s]\n","Train Epoch: 2/200 [78400/79999 (100%)]  Loss: 246.409735: 100%|██████████| 1600/1600 [00:07<00:00, 225.32it/s]\n","Train Epoch: 3/200 [78400/79999 (100%)]  Loss: 148.472836: 100%|██████████| 1600/1600 [00:06<00:00, 231.12it/s]\n","Train Epoch: 4/200 [78400/79999 (100%)]  Loss: 119.987252: 100%|██████████| 1600/1600 [00:07<00:00, 228.55it/s]\n","Train Epoch: 5/200 [78400/79999 (100%)]  Loss: 108.362268: 100%|██████████| 1600/1600 [00:06<00:00, 231.25it/s]\n","Train Epoch: 6/200 [78400/79999 (100%)]  Loss: 96.343384: 100%|██████████| 1600/1600 [00:06<00:00, 230.57it/s]\n","Train Epoch: 7/200 [78400/79999 (100%)]  Loss: 93.572859: 100%|██████████| 1600/1600 [00:07<00:00, 227.37it/s]\n","Train Epoch: 8/200 [78400/79999 (100%)]  Loss: 87.779109: 100%|██████████| 1600/1600 [00:06<00:00, 231.09it/s]\n","Train Epoch: 9/200 [78400/79999 (100%)]  Loss: 85.029936: 100%|██████████| 1600/1600 [00:06<00:00, 229.21it/s]\n","Train Epoch: 10/200 [78400/79999 (100%)]  Loss: 82.368789: 100%|██████████| 1600/1600 [00:06<00:00, 229.68it/s]\n","Train Epoch: 11/200 [78400/79999 (100%)]  Loss: 80.471489: 100%|██████████| 1600/1600 [00:06<00:00, 230.32it/s]\n","Train Epoch: 12/200 [78400/79999 (100%)]  Loss: 79.040641: 100%|██████████| 1600/1600 [00:06<00:00, 229.73it/s]\n","Train Epoch: 13/200 [78400/79999 (100%)]  Loss: 76.563897: 100%|██████████| 1600/1600 [00:06<00:00, 230.61it/s]\n","Train Epoch: 14/200 [78400/79999 (100%)]  Loss: 73.641658: 100%|██████████| 1600/1600 [00:06<00:00, 230.44it/s]\n","Train Epoch: 15/200 [78400/79999 (100%)]  Loss: 72.476929: 100%|██████████| 1600/1600 [00:07<00:00, 227.63it/s]\n","Train Epoch: 16/200 [78400/79999 (100%)]  Loss: 68.822272: 100%|██████████| 1600/1600 [00:06<00:00, 231.04it/s]\n","Train Epoch: 17/200 [78400/79999 (100%)]  Loss: 66.648631: 100%|██████████| 1600/1600 [00:06<00:00, 231.19it/s]\n","Train Epoch: 18/200 [78400/79999 (100%)]  Loss: 65.815271: 100%|██████████| 1600/1600 [00:06<00:00, 230.64it/s]\n","Train Epoch: 19/200 [78400/79999 (100%)]  Loss: 63.518960: 100%|██████████| 1600/1600 [00:06<00:00, 231.01it/s]\n","Train Epoch: 20/200 [78400/79999 (100%)]  Loss: 62.700357: 100%|██████████| 1600/1600 [00:06<00:00, 230.60it/s]\n","Train Epoch: 21/200 [78400/79999 (100%)]  Loss: 61.792249: 100%|██████████| 1600/1600 [00:06<00:00, 230.05it/s]\n","Train Epoch: 22/200 [78400/79999 (100%)]  Loss: 60.770025: 100%|██████████| 1600/1600 [00:06<00:00, 229.27it/s]\n","Train Epoch: 23/200 [78400/79999 (100%)]  Loss: 61.414659: 100%|██████████| 1600/1600 [00:06<00:00, 230.37it/s]\n","Train Epoch: 24/200 [78400/79999 (100%)]  Loss: 60.185519: 100%|██████████| 1600/1600 [00:07<00:00, 226.48it/s]\n","Train Epoch: 25/200 [78400/79999 (100%)]  Loss: 59.669242: 100%|██████████| 1600/1600 [00:06<00:00, 230.30it/s]\n","Train Epoch: 26/200 [78400/79999 (100%)]  Loss: 58.857564: 100%|██████████| 1600/1600 [00:06<00:00, 230.05it/s]\n","Train Epoch: 27/200 [78400/79999 (100%)]  Loss: 57.752427: 100%|██████████| 1600/1600 [00:07<00:00, 224.78it/s]\n","Train Epoch: 28/200 [78400/79999 (100%)]  Loss: 57.929138: 100%|██████████| 1600/1600 [00:07<00:00, 227.83it/s]\n","Train Epoch: 29/200 [78400/79999 (100%)]  Loss: 58.706491: 100%|██████████| 1600/1600 [00:07<00:00, 228.12it/s]\n","Train Epoch: 30/200 [78400/79999 (100%)]  Loss: 56.781938: 100%|██████████| 1600/1600 [00:07<00:00, 227.34it/s]\n","Train Epoch: 31/200 [78400/79999 (100%)]  Loss: 56.432380: 100%|██████████| 1600/1600 [00:06<00:00, 229.58it/s]\n","Train Epoch: 32/200 [78400/79999 (100%)]  Loss: 57.262067: 100%|██████████| 1600/1600 [00:06<00:00, 231.14it/s]\n","Train Epoch: 33/200 [78400/79999 (100%)]  Loss: 56.065648: 100%|██████████| 1600/1600 [00:07<00:00, 228.35it/s]\n","Train Epoch: 34/200 [78400/79999 (100%)]  Loss: 55.427217: 100%|██████████| 1600/1600 [00:06<00:00, 229.57it/s]\n","Train Epoch: 35/200 [78400/79999 (100%)]  Loss: 55.539877: 100%|██████████| 1600/1600 [00:06<00:00, 230.75it/s]\n","Train Epoch: 36/200 [78400/79999 (100%)]  Loss: 55.057362: 100%|██████████| 1600/1600 [00:06<00:00, 230.87it/s]\n","Train Epoch: 37/200 [78400/79999 (100%)]  Loss: 55.167230: 100%|██████████| 1600/1600 [00:06<00:00, 231.22it/s]\n","Train Epoch: 38/200 [78400/79999 (100%)]  Loss: 55.005364: 100%|██████████| 1600/1600 [00:06<00:00, 230.20it/s]\n","Train Epoch: 39/200 [78400/79999 (100%)]  Loss: 54.383869: 100%|██████████| 1600/1600 [00:06<00:00, 230.04it/s]\n","Train Epoch: 40/200 [78400/79999 (100%)]  Loss: 54.251296: 100%|██████████| 1600/1600 [00:06<00:00, 230.93it/s]\n","Train Epoch: 41/200 [78400/79999 (100%)]  Loss: 53.630722: 100%|██████████| 1600/1600 [00:07<00:00, 227.69it/s]\n","Train Epoch: 42/200 [78400/79999 (100%)]  Loss: 54.026761: 100%|██████████| 1600/1600 [00:06<00:00, 229.94it/s]\n","Train Epoch: 43/200 [78400/79999 (100%)]  Loss: 54.229437: 100%|██████████| 1600/1600 [00:07<00:00, 228.26it/s]\n","Train Epoch: 44/200 [78400/79999 (100%)]  Loss: 53.145233: 100%|██████████| 1600/1600 [00:06<00:00, 228.85it/s]\n","Train Epoch: 45/200 [78400/79999 (100%)]  Loss: 52.892460: 100%|██████████| 1600/1600 [00:06<00:00, 229.25it/s]\n","Train Epoch: 46/200 [78400/79999 (100%)]  Loss: 52.659316: 100%|██████████| 1600/1600 [00:06<00:00, 230.30it/s]\n","Train Epoch: 47/200 [78400/79999 (100%)]  Loss: 52.568498: 100%|██████████| 1600/1600 [00:06<00:00, 229.59it/s]\n","Train Epoch: 48/200 [78400/79999 (100%)]  Loss: 52.193975: 100%|██████████| 1600/1600 [00:06<00:00, 230.66it/s]\n","Train Epoch: 49/200 [78400/79999 (100%)]  Loss: 52.236054: 100%|██████████| 1600/1600 [00:06<00:00, 230.91it/s]\n","Train Epoch: 50/200 [78400/79999 (100%)]  Loss: 51.623337: 100%|██████████| 1600/1600 [00:06<00:00, 229.40it/s]\n","Train Epoch: 51/200 [78400/79999 (100%)]  Loss: 51.242183: 100%|██████████| 1600/1600 [00:06<00:00, 229.44it/s]\n","Train Epoch: 52/200 [78400/79999 (100%)]  Loss: 50.557182: 100%|██████████| 1600/1600 [00:06<00:00, 230.83it/s]\n","Train Epoch: 53/200 [78400/79999 (100%)]  Loss: 51.041689: 100%|██████████| 1600/1600 [00:06<00:00, 229.27it/s]\n","Train Epoch: 54/200 [78400/79999 (100%)]  Loss: 53.286608: 100%|██████████| 1600/1600 [00:07<00:00, 228.33it/s]\n","Train Epoch: 55/200 [78400/79999 (100%)]  Loss: 50.878640: 100%|██████████| 1600/1600 [00:07<00:00, 228.55it/s]\n","Train Epoch: 56/200 [78400/79999 (100%)]  Loss: 50.552652: 100%|██████████| 1600/1600 [00:06<00:00, 229.16it/s]\n","Train Epoch: 57/200 [78400/79999 (100%)]  Loss: 50.674822: 100%|██████████| 1600/1600 [00:06<00:00, 228.81it/s]\n","Train Epoch: 58/200 [78400/79999 (100%)]  Loss: 49.950448: 100%|██████████| 1600/1600 [00:07<00:00, 225.42it/s]\n","Train Epoch: 59/200 [78400/79999 (100%)]  Loss: 50.388148: 100%|██████████| 1600/1600 [00:06<00:00, 229.42it/s]\n","Train Epoch: 60/200 [78400/79999 (100%)]  Loss: 50.306602: 100%|██████████| 1600/1600 [00:06<00:00, 229.04it/s]\n","Train Epoch: 61/200 [78400/79999 (100%)]  Loss: 50.206374: 100%|██████████| 1600/1600 [00:06<00:00, 230.14it/s]\n","Train Epoch: 62/200 [78400/79999 (100%)]  Loss: 50.344643: 100%|██████████| 1600/1600 [00:07<00:00, 227.62it/s]\n","Train Epoch: 63/200 [78400/79999 (100%)]  Loss: 49.286982: 100%|██████████| 1600/1600 [00:07<00:00, 228.15it/s]\n","Train Epoch: 64/200 [78400/79999 (100%)]  Loss: 49.046481: 100%|██████████| 1600/1600 [00:06<00:00, 230.21it/s]\n","Train Epoch: 65/200 [78400/79999 (100%)]  Loss: 50.222600: 100%|██████████| 1600/1600 [00:06<00:00, 230.61it/s]\n","Train Epoch: 66/200 [78400/79999 (100%)]  Loss: 50.147938: 100%|██████████| 1600/1600 [00:06<00:00, 230.08it/s]\n","Train Epoch: 67/200 [78400/79999 (100%)]  Loss: 49.411206: 100%|██████████| 1600/1600 [00:07<00:00, 227.96it/s]\n","Train Epoch: 68/200 [78400/79999 (100%)]  Loss: 49.225350: 100%|██████████| 1600/1600 [00:06<00:00, 229.65it/s]\n","Train Epoch: 69/200 [78400/79999 (100%)]  Loss: 49.456007: 100%|██████████| 1600/1600 [00:06<00:00, 229.58it/s]\n","Train Epoch: 70/200 [78400/79999 (100%)]  Loss: 49.349977: 100%|██████████| 1600/1600 [00:06<00:00, 230.20it/s]\n","Train Epoch: 71/200 [78400/79999 (100%)]  Loss: 49.458164: 100%|██████████| 1600/1600 [00:06<00:00, 230.18it/s]\n","Train Epoch: 72/200 [78400/79999 (100%)]  Loss: 48.810074: 100%|██████████| 1600/1600 [00:06<00:00, 229.53it/s]\n","Train Epoch: 73/200 [78400/79999 (100%)]  Loss: 48.546242: 100%|██████████| 1600/1600 [00:06<00:00, 229.43it/s]\n","Train Epoch: 74/200 [78400/79999 (100%)]  Loss: 48.609641: 100%|██████████| 1600/1600 [00:06<00:00, 229.47it/s]\n","Train Epoch: 75/200 [78400/79999 (100%)]  Loss: 49.920291: 100%|██████████| 1600/1600 [00:06<00:00, 230.02it/s]\n","Train Epoch: 76/200 [78400/79999 (100%)]  Loss: 48.573688: 100%|██████████| 1600/1600 [00:07<00:00, 227.14it/s]\n","Train Epoch: 77/200 [78400/79999 (100%)]  Loss: 48.555816: 100%|██████████| 1600/1600 [00:07<00:00, 227.88it/s]\n","Train Epoch: 78/200 [78400/79999 (100%)]  Loss: 48.870050: 100%|██████████| 1600/1600 [00:07<00:00, 228.54it/s]\n","Train Epoch: 79/200 [78400/79999 (100%)]  Loss: 48.115907: 100%|██████████| 1600/1600 [00:06<00:00, 228.60it/s]\n","Train Epoch: 80/200 [78400/79999 (100%)]  Loss: 48.705916: 100%|██████████| 1600/1600 [00:06<00:00, 230.13it/s]\n","Train Epoch: 81/200 [78400/79999 (100%)]  Loss: 47.785501: 100%|██████████| 1600/1600 [00:06<00:00, 228.88it/s]\n","Train Epoch: 82/200 [78400/79999 (100%)]  Loss: 47.484479: 100%|██████████| 1600/1600 [00:06<00:00, 229.69it/s]\n","Train Epoch: 83/200 [78400/79999 (100%)]  Loss: 47.728677: 100%|██████████| 1600/1600 [00:06<00:00, 230.73it/s]\n","Train Epoch: 84/200 [78400/79999 (100%)]  Loss: 47.950563: 100%|██████████| 1600/1600 [00:07<00:00, 227.22it/s]\n","Train Epoch: 85/200 [78400/79999 (100%)]  Loss: 48.300710: 100%|██████████| 1600/1600 [00:06<00:00, 229.79it/s]\n","Train Epoch: 86/200 [78400/79999 (100%)]  Loss: 47.484925: 100%|██████████| 1600/1600 [00:06<00:00, 229.22it/s]\n","Train Epoch: 87/200 [78400/79999 (100%)]  Loss: 48.469216: 100%|██████████| 1600/1600 [00:06<00:00, 230.13it/s]\n","Train Epoch: 88/200 [78400/79999 (100%)]  Loss: 47.545561: 100%|██████████| 1600/1600 [00:06<00:00, 230.48it/s]\n","Train Epoch: 89/200 [78400/79999 (100%)]  Loss: 47.496850: 100%|██████████| 1600/1600 [00:06<00:00, 229.43it/s]\n","Train Epoch: 90/200 [78400/79999 (100%)]  Loss: 47.110932: 100%|██████████| 1600/1600 [00:06<00:00, 230.21it/s]\n","Train Epoch: 91/200 [78400/79999 (100%)]  Loss: 47.680040: 100%|██████████| 1600/1600 [00:06<00:00, 229.68it/s]\n","Train Epoch: 92/200 [78400/79999 (100%)]  Loss: 47.413802: 100%|██████████| 1600/1600 [00:06<00:00, 229.85it/s]\n","Train Epoch: 93/200 [78400/79999 (100%)]  Loss: 47.333761: 100%|██████████| 1600/1600 [00:06<00:00, 228.82it/s]\n","Train Epoch: 94/200 [78400/79999 (100%)]  Loss: 47.255021: 100%|██████████| 1600/1600 [00:06<00:00, 229.38it/s]\n","Train Epoch: 95/200 [78400/79999 (100%)]  Loss: 47.156170: 100%|██████████| 1600/1600 [00:06<00:00, 229.16it/s]\n","Train Epoch: 96/200 [78400/79999 (100%)]  Loss: 47.533154: 100%|██████████| 1600/1600 [00:06<00:00, 230.34it/s]\n","Train Epoch: 97/200 [78400/79999 (100%)]  Loss: 46.874961: 100%|██████████| 1600/1600 [00:06<00:00, 229.71it/s]\n","Train Epoch: 98/200 [78400/79999 (100%)]  Loss: 46.879666: 100%|██████████| 1600/1600 [00:07<00:00, 227.99it/s]\n","Train Epoch: 99/200 [78400/79999 (100%)]  Loss: 47.293451: 100%|██████████| 1600/1600 [00:07<00:00, 228.19it/s]\n","Train Epoch: 100/200 [78400/79999 (100%)]  Loss: 46.838240: 100%|██████████| 1600/1600 [00:07<00:00, 227.24it/s]\n","Train Epoch: 101/200 [78400/79999 (100%)]  Loss: 47.864373: 100%|██████████| 1600/1600 [00:07<00:00, 227.50it/s]\n","Train Epoch: 102/200 [78400/79999 (100%)]  Loss: 46.247175: 100%|██████████| 1600/1600 [00:06<00:00, 229.36it/s]\n","Train Epoch: 103/200 [78400/79999 (100%)]  Loss: 46.717552: 100%|██████████| 1600/1600 [00:06<00:00, 229.35it/s]\n","Train Epoch: 104/200 [78400/79999 (100%)]  Loss: 45.535059: 100%|██████████| 1600/1600 [00:07<00:00, 227.54it/s]\n","Train Epoch: 105/200 [78400/79999 (100%)]  Loss: 46.280332: 100%|██████████| 1600/1600 [00:07<00:00, 226.25it/s]\n","Train Epoch: 106/200 [78400/79999 (100%)]  Loss: 46.126061: 100%|██████████| 1600/1600 [00:07<00:00, 223.60it/s]\n","Train Epoch: 107/200 [78400/79999 (100%)]  Loss: 46.141245: 100%|██████████| 1600/1600 [00:06<00:00, 228.89it/s]\n","Train Epoch: 108/200 [78400/79999 (100%)]  Loss: 45.776872: 100%|██████████| 1600/1600 [00:07<00:00, 227.27it/s]\n","Train Epoch: 109/200 [78400/79999 (100%)]  Loss: 46.324739: 100%|██████████| 1600/1600 [00:06<00:00, 228.66it/s]\n","Train Epoch: 110/200 [78400/79999 (100%)]  Loss: 47.912957: 100%|██████████| 1600/1600 [00:07<00:00, 226.63it/s]\n","Train Epoch: 111/200 [78400/79999 (100%)]  Loss: 46.283345: 100%|██████████| 1600/1600 [00:06<00:00, 228.75it/s]\n","Train Epoch: 112/200 [78400/79999 (100%)]  Loss: 45.558215: 100%|██████████| 1600/1600 [00:06<00:00, 229.74it/s]\n","Train Epoch: 113/200 [78400/79999 (100%)]  Loss: 44.844630: 100%|██████████| 1600/1600 [00:06<00:00, 228.91it/s]\n","Train Epoch: 114/200 [78400/79999 (100%)]  Loss: 46.163834: 100%|██████████| 1600/1600 [00:07<00:00, 226.96it/s]\n","Train Epoch: 115/200 [78400/79999 (100%)]  Loss: 46.271966: 100%|██████████| 1600/1600 [00:07<00:00, 227.83it/s]\n","Train Epoch: 116/200 [78400/79999 (100%)]  Loss: 44.940502: 100%|██████████| 1600/1600 [00:07<00:00, 227.61it/s]\n","Train Epoch: 117/200 [78400/79999 (100%)]  Loss: 45.727753: 100%|██████████| 1600/1600 [00:06<00:00, 228.61it/s]\n","Train Epoch: 118/200 [78400/79999 (100%)]  Loss: 45.455691: 100%|██████████| 1600/1600 [00:07<00:00, 227.37it/s]\n","Train Epoch: 119/200 [78400/79999 (100%)]  Loss: 45.188245: 100%|██████████| 1600/1600 [00:06<00:00, 228.75it/s]\n","Train Epoch: 120/200 [78400/79999 (100%)]  Loss: 45.382563: 100%|██████████| 1600/1600 [00:06<00:00, 230.20it/s]\n","Train Epoch: 121/200 [78400/79999 (100%)]  Loss: 44.401147: 100%|██████████| 1600/1600 [00:06<00:00, 230.27it/s]\n","Train Epoch: 122/200 [78400/79999 (100%)]  Loss: 45.135441: 100%|██████████| 1600/1600 [00:07<00:00, 227.41it/s]\n","Train Epoch: 123/200 [78400/79999 (100%)]  Loss: 45.344227: 100%|██████████| 1600/1600 [00:06<00:00, 229.87it/s]\n","Train Epoch: 124/200 [78400/79999 (100%)]  Loss: 44.953337: 100%|██████████| 1600/1600 [00:06<00:00, 228.70it/s]\n","Train Epoch: 125/200 [78400/79999 (100%)]  Loss: 44.328471: 100%|██████████| 1600/1600 [00:06<00:00, 229.29it/s]\n","Train Epoch: 126/200 [78400/79999 (100%)]  Loss: 44.132705: 100%|██████████| 1600/1600 [00:06<00:00, 229.97it/s]\n","Train Epoch: 127/200 [78400/79999 (100%)]  Loss: 45.051095: 100%|██████████| 1600/1600 [00:07<00:00, 227.55it/s]\n","Train Epoch: 128/200 [78400/79999 (100%)]  Loss: 43.936331: 100%|██████████| 1600/1600 [00:06<00:00, 229.67it/s]\n","Train Epoch: 129/200 [78400/79999 (100%)]  Loss: 44.551724: 100%|██████████| 1600/1600 [00:06<00:00, 230.60it/s]\n","Train Epoch: 130/200 [78400/79999 (100%)]  Loss: 45.362619: 100%|██████████| 1600/1600 [00:06<00:00, 229.39it/s]\n","Train Epoch: 131/200 [78400/79999 (100%)]  Loss: 44.172390: 100%|██████████| 1600/1600 [00:06<00:00, 229.34it/s]\n","Train Epoch: 132/200 [78400/79999 (100%)]  Loss: 43.796377: 100%|██████████| 1600/1600 [00:06<00:00, 228.84it/s]\n","Train Epoch: 133/200 [78400/79999 (100%)]  Loss: 44.303590: 100%|██████████| 1600/1600 [00:07<00:00, 228.56it/s]\n","Train Epoch: 134/200 [78400/79999 (100%)]  Loss: 44.541887: 100%|██████████| 1600/1600 [00:06<00:00, 229.13it/s]\n","Train Epoch: 135/200 [78400/79999 (100%)]  Loss: 43.521903: 100%|██████████| 1600/1600 [00:07<00:00, 227.68it/s]\n","Train Epoch: 136/200 [78400/79999 (100%)]  Loss: 44.171658: 100%|██████████| 1600/1600 [00:07<00:00, 227.96it/s]\n","Train Epoch: 137/200 [78400/79999 (100%)]  Loss: 43.655468: 100%|██████████| 1600/1600 [00:06<00:00, 229.35it/s]\n","Train Epoch: 138/200 [78400/79999 (100%)]  Loss: 43.530561: 100%|██████████| 1600/1600 [00:07<00:00, 227.77it/s]\n","Train Epoch: 139/200 [78400/79999 (100%)]  Loss: 43.816404: 100%|██████████| 1600/1600 [00:07<00:00, 227.94it/s]\n","Train Epoch: 140/200 [78400/79999 (100%)]  Loss: 44.035351: 100%|██████████| 1600/1600 [00:07<00:00, 228.43it/s]\n","Train Epoch: 141/200 [78400/79999 (100%)]  Loss: 42.893549: 100%|██████████| 1600/1600 [00:06<00:00, 229.10it/s]\n","Train Epoch: 142/200 [78400/79999 (100%)]  Loss: 43.664001: 100%|██████████| 1600/1600 [00:06<00:00, 229.28it/s]\n","Train Epoch: 143/200 [78400/79999 (100%)]  Loss: 43.214736: 100%|██████████| 1600/1600 [00:07<00:00, 225.93it/s]\n","Train Epoch: 144/200 [78400/79999 (100%)]  Loss: 42.711679: 100%|██████████| 1600/1600 [00:07<00:00, 226.42it/s]\n","Train Epoch: 145/200 [78400/79999 (100%)]  Loss: 42.892621: 100%|██████████| 1600/1600 [00:06<00:00, 229.05it/s]\n","Train Epoch: 146/200 [78400/79999 (100%)]  Loss: 42.771323: 100%|██████████| 1600/1600 [00:06<00:00, 229.35it/s]\n","Train Epoch: 147/200 [78400/79999 (100%)]  Loss: 43.105932: 100%|██████████| 1600/1600 [00:06<00:00, 229.29it/s]\n","Train Epoch: 148/200 [78400/79999 (100%)]  Loss: 42.136392: 100%|██████████| 1600/1600 [00:06<00:00, 228.72it/s]\n","Train Epoch: 149/200 [78400/79999 (100%)]  Loss: 42.583018: 100%|██████████| 1600/1600 [00:06<00:00, 229.44it/s]\n","Train Epoch: 150/200 [78400/79999 (100%)]  Loss: 42.059654: 100%|██████████| 1600/1600 [00:06<00:00, 228.79it/s]\n","Train Epoch: 151/200 [78400/79999 (100%)]  Loss: 41.877333: 100%|██████████| 1600/1600 [00:06<00:00, 230.11it/s]\n","Train Epoch: 152/200 [78400/79999 (100%)]  Loss: 42.033897: 100%|██████████| 1600/1600 [00:06<00:00, 229.75it/s]\n","Train Epoch: 153/200 [78400/79999 (100%)]  Loss: 41.582560: 100%|██████████| 1600/1600 [00:07<00:00, 226.97it/s]\n","Train Epoch: 154/200 [78400/79999 (100%)]  Loss: 42.192234: 100%|██████████| 1600/1600 [00:06<00:00, 228.94it/s]\n","Train Epoch: 155/200 [78400/79999 (100%)]  Loss: 41.424713: 100%|██████████| 1600/1600 [00:06<00:00, 229.26it/s]\n","Train Epoch: 156/200 [78400/79999 (100%)]  Loss: 42.839273: 100%|██████████| 1600/1600 [00:06<00:00, 230.32it/s]\n","Train Epoch: 157/200 [78400/79999 (100%)]  Loss: 41.727976: 100%|██████████| 1600/1600 [00:07<00:00, 227.47it/s]\n","Train Epoch: 158/200 [78400/79999 (100%)]  Loss: 41.203521: 100%|██████████| 1600/1600 [00:06<00:00, 229.03it/s]\n","Train Epoch: 159/200 [78400/79999 (100%)]  Loss: 41.239004: 100%|██████████| 1600/1600 [00:06<00:00, 229.50it/s]\n","Train Epoch: 160/200 [78400/79999 (100%)]  Loss: 41.520492: 100%|██████████| 1600/1600 [00:06<00:00, 229.07it/s]\n","Train Epoch: 161/200 [78400/79999 (100%)]  Loss: 41.553324: 100%|██████████| 1600/1600 [00:07<00:00, 227.47it/s]\n","Train Epoch: 162/200 [78400/79999 (100%)]  Loss: 41.053300: 100%|██████████| 1600/1600 [00:06<00:00, 228.58it/s]\n","Train Epoch: 163/200 [78400/79999 (100%)]  Loss: 41.178026: 100%|██████████| 1600/1600 [00:06<00:00, 228.95it/s]\n","Train Epoch: 164/200 [78400/79999 (100%)]  Loss: 40.650287: 100%|██████████| 1600/1600 [00:06<00:00, 229.89it/s]\n","Train Epoch: 165/200 [78400/79999 (100%)]  Loss: 42.063910: 100%|██████████| 1600/1600 [00:06<00:00, 228.64it/s]\n","Train Epoch: 166/200 [78400/79999 (100%)]  Loss: 40.552488: 100%|██████████| 1600/1600 [00:06<00:00, 229.32it/s]\n","Train Epoch: 167/200 [78400/79999 (100%)]  Loss: 40.522548: 100%|██████████| 1600/1600 [00:06<00:00, 228.95it/s]\n","Train Epoch: 168/200 [78400/79999 (100%)]  Loss: 40.718520: 100%|██████████| 1600/1600 [00:07<00:00, 228.45it/s]\n","Train Epoch: 169/200 [78400/79999 (100%)]  Loss: 40.297288: 100%|██████████| 1600/1600 [00:06<00:00, 229.34it/s]\n","Train Epoch: 170/200 [78400/79999 (100%)]  Loss: 40.379868: 100%|██████████| 1600/1600 [00:07<00:00, 226.80it/s]\n","Train Epoch: 171/200 [78400/79999 (100%)]  Loss: 40.290030: 100%|██████████| 1600/1600 [00:07<00:00, 227.96it/s]\n","Train Epoch: 172/200 [78400/79999 (100%)]  Loss: 39.716428: 100%|██████████| 1600/1600 [00:06<00:00, 228.64it/s]\n","Train Epoch: 173/200 [78400/79999 (100%)]  Loss: 40.603388: 100%|██████████| 1600/1600 [00:06<00:00, 228.80it/s]\n","Train Epoch: 174/200 [78400/79999 (100%)]  Loss: 40.788427: 100%|██████████| 1600/1600 [00:06<00:00, 229.41it/s]\n","Train Epoch: 175/200 [78400/79999 (100%)]  Loss: 40.238664: 100%|██████████| 1600/1600 [00:06<00:00, 229.72it/s]\n","Train Epoch: 176/200 [78400/79999 (100%)]  Loss: 40.341683: 100%|██████████| 1600/1600 [00:06<00:00, 229.92it/s]\n","Train Epoch: 177/200 [78400/79999 (100%)]  Loss: 40.770387: 100%|██████████| 1600/1600 [00:07<00:00, 228.53it/s]\n","Train Epoch: 178/200 [78400/79999 (100%)]  Loss: 40.221219: 100%|██████████| 1600/1600 [00:07<00:00, 226.82it/s]\n","Train Epoch: 179/200 [78400/79999 (100%)]  Loss: 39.633349: 100%|██████████| 1600/1600 [00:06<00:00, 229.75it/s]\n","Train Epoch: 180/200 [78400/79999 (100%)]  Loss: 40.082475: 100%|██████████| 1600/1600 [00:06<00:00, 229.04it/s]\n","Train Epoch: 181/200 [78400/79999 (100%)]  Loss: 39.971598: 100%|██████████| 1600/1600 [00:06<00:00, 228.76it/s]\n","Train Epoch: 182/200 [78400/79999 (100%)]  Loss: 39.480192: 100%|██████████| 1600/1600 [00:06<00:00, 228.95it/s]\n","Train Epoch: 183/200 [78400/79999 (100%)]  Loss: 39.776665: 100%|██████████| 1600/1600 [00:06<00:00, 229.34it/s]\n","Train Epoch: 184/200 [78400/79999 (100%)]  Loss: 40.185564: 100%|██████████| 1600/1600 [00:07<00:00, 225.27it/s]\n","Train Epoch: 185/200 [78400/79999 (100%)]  Loss: 39.775387: 100%|██████████| 1600/1600 [00:07<00:00, 225.60it/s]\n","Train Epoch: 186/200 [78400/79999 (100%)]  Loss: 39.848421: 100%|██████████| 1600/1600 [00:07<00:00, 225.91it/s]\n","Train Epoch: 187/200 [78400/79999 (100%)]  Loss: 40.493557: 100%|██████████| 1600/1600 [00:07<00:00, 223.52it/s]\n","Train Epoch: 188/200 [78400/79999 (100%)]  Loss: 40.368865: 100%|██████████| 1600/1600 [00:07<00:00, 227.32it/s]\n","Train Epoch: 189/200 [78400/79999 (100%)]  Loss: 39.797294: 100%|██████████| 1600/1600 [00:06<00:00, 228.88it/s]\n","Train Epoch: 190/200 [78400/79999 (100%)]  Loss: 39.229576: 100%|██████████| 1600/1600 [00:06<00:00, 229.35it/s]\n","Train Epoch: 191/200 [78400/79999 (100%)]  Loss: 39.973915: 100%|██████████| 1600/1600 [00:06<00:00, 228.68it/s]\n","Train Epoch: 192/200 [78400/79999 (100%)]  Loss: 39.586639: 100%|██████████| 1600/1600 [00:07<00:00, 228.06it/s]\n","Train Epoch: 193/200 [78400/79999 (100%)]  Loss: 40.028161: 100%|██████████| 1600/1600 [00:06<00:00, 228.81it/s]\n","Train Epoch: 194/200 [78400/79999 (100%)]  Loss: 39.408633: 100%|██████████| 1600/1600 [00:06<00:00, 229.16it/s]\n","Train Epoch: 195/200 [78400/79999 (100%)]  Loss: 40.135129: 100%|██████████| 1600/1600 [00:07<00:00, 227.50it/s]\n","Train Epoch: 196/200 [78400/79999 (100%)]  Loss: 39.464365: 100%|██████████| 1600/1600 [00:06<00:00, 229.34it/s]\n","Train Epoch: 197/200 [78400/79999 (100%)]  Loss: 39.621301: 100%|██████████| 1600/1600 [00:07<00:00, 228.49it/s]\n","Train Epoch: 198/200 [78400/79999 (100%)]  Loss: 39.537017: 100%|██████████| 1600/1600 [00:07<00:00, 227.94it/s]\n","Train Epoch: 199/200 [78400/79999 (100%)]  Loss: 39.136840: 100%|██████████| 1600/1600 [00:07<00:00, 227.70it/s]\n","Train Epoch: 200/200 [78400/79999 (100%)]  Loss: 39.505553: 100%|██████████| 1600/1600 [00:07<00:00, 227.02it/s]\n"]}]},{"cell_type":"code","source":["from utils import load_data, load_model\n","test_path = '../datasets/Spirit/chronological_test.csv'\n","split = 50\n","\n","fi = pd.read_csv('../datasets/Spirit/pca_vector.csv', header = None)\n","vec = []\n","vec = fi\n","vec = np.array(vec)\n","\n","test_logs_series = pd.read_csv(test_path)\n","test_logs_series = test_logs_series.values\n","test_total = len(test_logs_series)\n","sub = int(test_total/split)\n","\n","\n","\n","def mod(l, n):\n","    \"\"\" Truncate or pad a list \"\"\"\n","    r = l[-1*n:]\n","    if len(r) < n:\n","        r.extend(list([0]) * (n - len(r)))\n","    return r\n","\n","def load_test(i):\n","    if i!=split-1:\n","        label = test_logs_series[i*sub:(i+1)*sub,1]\n","        logs_data = test_logs_series[i*sub:(i+1)*sub,0]\n","    else:\n","        label = test_logs_series[i*sub:,1]\n","        logs_data = test_logs_series[i*sub:,0]\n","    logs = []\n","\n","    for logid in range(0,len(logs_data)):\n","        ori_seq = [\n","            int(eventid) for eventid in logs_data[logid].split()]\n","        seq_pattern = mod(ori_seq, sequence_length)\n","        vec_pattern = []\n","\n","        for event in seq_pattern:\n","            if event == 0:\n","                vec_pattern.append([-1]*input_size)\n","            else:\n","                vec_pattern.append(vec[event-1])  \n","        logs.append(vec_pattern)\n","    logs = np.array(logs)\n","    train_x = logs\n","    train_y = label\n","    train_x = np.reshape(train_x, (train_x.shape[0], -1, input_size))\n","    train_y = train_y.astype(int)\n","    return train_x, train_y\n","\n","\n","\n","def test(model, criterion = nn.CrossEntropyLoss()):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        TP = 0 \n","        FP = 0\n","        FN = 0 \n","        TN = 0\n","        for i in range (0, split):        #################################################\n","            test_x, test_y = load_test(i)\n","            test_loader = load_data(test_x, test_y, batch_size)            \n","            for data, target in test_loader:\n","                data, target = data.to(device), target.to(device)\n","                output, _ = model(data)\n","                test_loss += criterion(output, target) # sum up batch loss\n","                \n","                output = torch.sigmoid(output)[:, 0].cpu().detach().numpy()\n","                predicted = (output < 0.2).astype(int)\n","                target = np.array([y.cpu() for y in target])\n","\n","                TP += ((predicted == 1) * (target == 1)).sum()\n","                FP += ((predicted == 1) * (target == 0)).sum()\n","                FN += ((predicted == 0) * (target == 1)).sum()\n","                TN += ((predicted == 0) * (target == 0)).sum()\n","        P = 100 * TP / (TP + FP)\n","        R = 100 * TP / (TP + FN)\n","        F1 = 2 * P * R / (P + R)   \n","        accuracy = 100 * (TP + TN)/(TP + TN + FP + FN)\n","        #MCC = 100*(TP*TN + FP*FN)/math.sqrt((TP+FP)*(TN+FN)*(TN+FP)*(TP+FN))         \n","    return accuracy, test_loss, P, R, F1, TP, FP, TN, FN\n","\n","Teacher = load_model(Teacher, save_teacher_path)\n","\n","\n","start_time = time()\n","accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(Teacher, criterion = nn.CrossEntropyLoss())\n","test_loss /= (split*sub)\n","\n","print('Result of testing teacher model')\n","print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n","print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%). Total time = {time() - start_time}')\n","print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%' .format(P, R, F1))\n"],"metadata":{"id":"ZP6hOJPm0YWr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c88a713b-dc77-496d-ea33-27196847801d","executionInfo":{"status":"ok","timestamp":1665721748477,"user_tz":-420,"elapsed":3295,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Result of testing teacher model\n","false positive (FP): 36, false negative (FN): 413, true positive (TP): 93, true negative (TN): 19457\n","Test set: Average loss: 0.0020, Accuracy: 97.75%). Total time = 2.636810064315796\n","Precision: 72.093%, Recall: 18.379%, F1-measure: 29.291%\n"]}]},{"cell_type":"code","source":["noKD = train(noKD, train_loader, learning_rate = 0.0003, num_epochs = 200)\n","save_model(noKD, save_noKD_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lAIpmyTUwQca","executionInfo":{"status":"ok","timestamp":1665722893999,"user_tz":-420,"elapsed":1110160,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"380d7a4a-1199-45f4-c85c-50a9f76d4413"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Train Epoch: 1/200 [78400/79999 (100%)]  Loss: 1064.930625: 100%|██████████| 1600/1600 [00:05<00:00, 267.58it/s]\n","Train Epoch: 2/200 [78400/79999 (100%)]  Loss: 1065.762394: 100%|██████████| 1600/1600 [00:05<00:00, 289.58it/s]\n","Train Epoch: 3/200 [78400/79999 (100%)]  Loss: 951.789428: 100%|██████████| 1600/1600 [00:05<00:00, 290.60it/s]\n","Train Epoch: 4/200 [78400/79999 (100%)]  Loss: 526.150686: 100%|██████████| 1600/1600 [00:05<00:00, 290.74it/s]\n","Train Epoch: 5/200 [78400/79999 (100%)]  Loss: 383.664132: 100%|██████████| 1600/1600 [00:05<00:00, 281.29it/s]\n","Train Epoch: 6/200 [78400/79999 (100%)]  Loss: 346.021092: 100%|██████████| 1600/1600 [00:05<00:00, 289.86it/s]\n","Train Epoch: 7/200 [78400/79999 (100%)]  Loss: 324.975238: 100%|██████████| 1600/1600 [00:05<00:00, 287.59it/s]\n","Train Epoch: 8/200 [78400/79999 (100%)]  Loss: 311.821755: 100%|██████████| 1600/1600 [00:05<00:00, 291.23it/s]\n","Train Epoch: 9/200 [78400/79999 (100%)]  Loss: 301.268683: 100%|██████████| 1600/1600 [00:05<00:00, 291.72it/s]\n","Train Epoch: 10/200 [78400/79999 (100%)]  Loss: 291.362828: 100%|██████████| 1600/1600 [00:05<00:00, 288.88it/s]\n","Train Epoch: 11/200 [78400/79999 (100%)]  Loss: 280.165767: 100%|██████████| 1600/1600 [00:05<00:00, 290.51it/s]\n","Train Epoch: 12/200 [78400/79999 (100%)]  Loss: 266.071899: 100%|██████████| 1600/1600 [00:05<00:00, 290.60it/s]\n","Train Epoch: 13/200 [78400/79999 (100%)]  Loss: 244.117654: 100%|██████████| 1600/1600 [00:05<00:00, 285.76it/s]\n","Train Epoch: 14/200 [78400/79999 (100%)]  Loss: 210.010608: 100%|██████████| 1600/1600 [00:05<00:00, 286.36it/s]\n","Train Epoch: 15/200 [78400/79999 (100%)]  Loss: 176.398219: 100%|██████████| 1600/1600 [00:05<00:00, 287.98it/s]\n","Train Epoch: 16/200 [78400/79999 (100%)]  Loss: 158.986913: 100%|██████████| 1600/1600 [00:05<00:00, 288.74it/s]\n","Train Epoch: 17/200 [78400/79999 (100%)]  Loss: 149.683726: 100%|██████████| 1600/1600 [00:05<00:00, 291.19it/s]\n","Train Epoch: 18/200 [78400/79999 (100%)]  Loss: 143.841418: 100%|██████████| 1600/1600 [00:05<00:00, 287.08it/s]\n","Train Epoch: 19/200 [78400/79999 (100%)]  Loss: 139.933188: 100%|██████████| 1600/1600 [00:05<00:00, 287.04it/s]\n","Train Epoch: 20/200 [78400/79999 (100%)]  Loss: 136.862803: 100%|██████████| 1600/1600 [00:05<00:00, 283.53it/s]\n","Train Epoch: 21/200 [78400/79999 (100%)]  Loss: 134.227160: 100%|██████████| 1600/1600 [00:05<00:00, 286.69it/s]\n","Train Epoch: 22/200 [78400/79999 (100%)]  Loss: 131.933163: 100%|██████████| 1600/1600 [00:05<00:00, 287.14it/s]\n","Train Epoch: 23/200 [78400/79999 (100%)]  Loss: 129.784717: 100%|██████████| 1600/1600 [00:05<00:00, 288.56it/s]\n","Train Epoch: 24/200 [78400/79999 (100%)]  Loss: 127.862114: 100%|██████████| 1600/1600 [00:05<00:00, 272.73it/s]\n","Train Epoch: 25/200 [78400/79999 (100%)]  Loss: 126.192783: 100%|██████████| 1600/1600 [00:05<00:00, 282.30it/s]\n","Train Epoch: 26/200 [78400/79999 (100%)]  Loss: 124.614544: 100%|██████████| 1600/1600 [00:05<00:00, 288.97it/s]\n","Train Epoch: 27/200 [78400/79999 (100%)]  Loss: 123.137439: 100%|██████████| 1600/1600 [00:05<00:00, 288.88it/s]\n","Train Epoch: 28/200 [78400/79999 (100%)]  Loss: 121.760193: 100%|██████████| 1600/1600 [00:05<00:00, 288.90it/s]\n","Train Epoch: 29/200 [78400/79999 (100%)]  Loss: 120.373773: 100%|██████████| 1600/1600 [00:05<00:00, 275.59it/s]\n","Train Epoch: 30/200 [78400/79999 (100%)]  Loss: 118.939559: 100%|██████████| 1600/1600 [00:05<00:00, 293.69it/s]\n","Train Epoch: 31/200 [78400/79999 (100%)]  Loss: 117.657107: 100%|██████████| 1600/1600 [00:05<00:00, 290.72it/s]\n","Train Epoch: 32/200 [78400/79999 (100%)]  Loss: 116.556241: 100%|██████████| 1600/1600 [00:05<00:00, 291.05it/s]\n","Train Epoch: 33/200 [78400/79999 (100%)]  Loss: 115.537093: 100%|██████████| 1600/1600 [00:05<00:00, 293.13it/s]\n","Train Epoch: 34/200 [78400/79999 (100%)]  Loss: 114.579404: 100%|██████████| 1600/1600 [00:05<00:00, 293.27it/s]\n","Train Epoch: 35/200 [78400/79999 (100%)]  Loss: 113.630642: 100%|██████████| 1600/1600 [00:05<00:00, 292.47it/s]\n","Train Epoch: 36/200 [78400/79999 (100%)]  Loss: 113.003803: 100%|██████████| 1600/1600 [00:05<00:00, 292.05it/s]\n","Train Epoch: 37/200 [78400/79999 (100%)]  Loss: 112.117225: 100%|██████████| 1600/1600 [00:05<00:00, 292.49it/s]\n","Train Epoch: 38/200 [78400/79999 (100%)]  Loss: 111.588201: 100%|██████████| 1600/1600 [00:05<00:00, 291.88it/s]\n","Train Epoch: 39/200 [78400/79999 (100%)]  Loss: 111.072649: 100%|██████████| 1600/1600 [00:05<00:00, 293.47it/s]\n","Train Epoch: 40/200 [78400/79999 (100%)]  Loss: 110.148883: 100%|██████████| 1600/1600 [00:05<00:00, 291.39it/s]\n","Train Epoch: 41/200 [78400/79999 (100%)]  Loss: 109.758044: 100%|██████████| 1600/1600 [00:05<00:00, 292.52it/s]\n","Train Epoch: 42/200 [78400/79999 (100%)]  Loss: 108.985632: 100%|██████████| 1600/1600 [00:05<00:00, 290.84it/s]\n","Train Epoch: 43/200 [78400/79999 (100%)]  Loss: 108.249617: 100%|██████████| 1600/1600 [00:05<00:00, 289.66it/s]\n","Train Epoch: 44/200 [78400/79999 (100%)]  Loss: 107.899541: 100%|██████████| 1600/1600 [00:05<00:00, 290.86it/s]\n","Train Epoch: 45/200 [78400/79999 (100%)]  Loss: 107.278900: 100%|██████████| 1600/1600 [00:05<00:00, 289.34it/s]\n","Train Epoch: 46/200 [78400/79999 (100%)]  Loss: 106.768326: 100%|██████████| 1600/1600 [00:05<00:00, 291.44it/s]\n","Train Epoch: 47/200 [78400/79999 (100%)]  Loss: 106.135211: 100%|██████████| 1600/1600 [00:05<00:00, 289.76it/s]\n","Train Epoch: 48/200 [78400/79999 (100%)]  Loss: 105.697620: 100%|██████████| 1600/1600 [00:05<00:00, 289.52it/s]\n","Train Epoch: 49/200 [78400/79999 (100%)]  Loss: 104.909351: 100%|██████████| 1600/1600 [00:05<00:00, 290.43it/s]\n","Train Epoch: 50/200 [78400/79999 (100%)]  Loss: 104.492724: 100%|██████████| 1600/1600 [00:05<00:00, 291.38it/s]\n","Train Epoch: 51/200 [78400/79999 (100%)]  Loss: 103.893860: 100%|██████████| 1600/1600 [00:05<00:00, 290.54it/s]\n","Train Epoch: 52/200 [78400/79999 (100%)]  Loss: 103.508900: 100%|██████████| 1600/1600 [00:05<00:00, 291.08it/s]\n","Train Epoch: 53/200 [78400/79999 (100%)]  Loss: 103.310060: 100%|██████████| 1600/1600 [00:05<00:00, 290.20it/s]\n","Train Epoch: 54/200 [78400/79999 (100%)]  Loss: 102.355767: 100%|██████████| 1600/1600 [00:05<00:00, 291.13it/s]\n","Train Epoch: 55/200 [78400/79999 (100%)]  Loss: 101.970394: 100%|██████████| 1600/1600 [00:05<00:00, 292.00it/s]\n","Train Epoch: 56/200 [78400/79999 (100%)]  Loss: 101.444381: 100%|██████████| 1600/1600 [00:05<00:00, 288.39it/s]\n","Train Epoch: 57/200 [78400/79999 (100%)]  Loss: 100.909904: 100%|██████████| 1600/1600 [00:05<00:00, 291.37it/s]\n","Train Epoch: 58/200 [78400/79999 (100%)]  Loss: 100.504415: 100%|██████████| 1600/1600 [00:05<00:00, 288.09it/s]\n","Train Epoch: 59/200 [78400/79999 (100%)]  Loss: 99.920046: 100%|██████████| 1600/1600 [00:05<00:00, 286.59it/s]\n","Train Epoch: 60/200 [78400/79999 (100%)]  Loss: 100.424599: 100%|██████████| 1600/1600 [00:05<00:00, 291.74it/s]\n","Train Epoch: 61/200 [78400/79999 (100%)]  Loss: 99.461259: 100%|██████████| 1600/1600 [00:05<00:00, 290.60it/s]\n","Train Epoch: 62/200 [78400/79999 (100%)]  Loss: 99.293505: 100%|██████████| 1600/1600 [00:05<00:00, 289.07it/s]\n","Train Epoch: 63/200 [78400/79999 (100%)]  Loss: 98.490225: 100%|██████████| 1600/1600 [00:05<00:00, 289.51it/s]\n","Train Epoch: 64/200 [78400/79999 (100%)]  Loss: 98.345005: 100%|██████████| 1600/1600 [00:05<00:00, 291.97it/s]\n","Train Epoch: 65/200 [78400/79999 (100%)]  Loss: 97.765799: 100%|██████████| 1600/1600 [00:05<00:00, 290.21it/s]\n","Train Epoch: 66/200 [78400/79999 (100%)]  Loss: 98.667728: 100%|██████████| 1600/1600 [00:05<00:00, 290.76it/s]\n","Train Epoch: 67/200 [78400/79999 (100%)]  Loss: 97.494037: 100%|██████████| 1600/1600 [00:05<00:00, 289.09it/s]\n","Train Epoch: 68/200 [78400/79999 (100%)]  Loss: 98.161969: 100%|██████████| 1600/1600 [00:05<00:00, 291.56it/s]\n","Train Epoch: 69/200 [78400/79999 (100%)]  Loss: 97.365948: 100%|██████████| 1600/1600 [00:05<00:00, 287.39it/s]\n","Train Epoch: 70/200 [78400/79999 (100%)]  Loss: 96.981152: 100%|██████████| 1600/1600 [00:05<00:00, 290.36it/s]\n","Train Epoch: 71/200 [78400/79999 (100%)]  Loss: 96.465232: 100%|██████████| 1600/1600 [00:05<00:00, 290.76it/s]\n","Train Epoch: 72/200 [78400/79999 (100%)]  Loss: 95.903439: 100%|██████████| 1600/1600 [00:05<00:00, 288.67it/s]\n","Train Epoch: 73/200 [78400/79999 (100%)]  Loss: 95.257719: 100%|██████████| 1600/1600 [00:05<00:00, 288.98it/s]\n","Train Epoch: 74/200 [78400/79999 (100%)]  Loss: 95.032334: 100%|██████████| 1600/1600 [00:05<00:00, 289.69it/s]\n","Train Epoch: 75/200 [78400/79999 (100%)]  Loss: 94.662602: 100%|██████████| 1600/1600 [00:05<00:00, 287.37it/s]\n","Train Epoch: 76/200 [78400/79999 (100%)]  Loss: 94.763208: 100%|██████████| 1600/1600 [00:05<00:00, 284.95it/s]\n","Train Epoch: 77/200 [78400/79999 (100%)]  Loss: 94.543577: 100%|██████████| 1600/1600 [00:05<00:00, 290.04it/s]\n","Train Epoch: 78/200 [78400/79999 (100%)]  Loss: 93.749370: 100%|██████████| 1600/1600 [00:05<00:00, 288.86it/s]\n","Train Epoch: 79/200 [78400/79999 (100%)]  Loss: 93.372714: 100%|██████████| 1600/1600 [00:05<00:00, 289.81it/s]\n","Train Epoch: 80/200 [78400/79999 (100%)]  Loss: 93.046335: 100%|██████████| 1600/1600 [00:05<00:00, 288.18it/s]\n","Train Epoch: 81/200 [78400/79999 (100%)]  Loss: 93.291835: 100%|██████████| 1600/1600 [00:05<00:00, 288.42it/s]\n","Train Epoch: 82/200 [78400/79999 (100%)]  Loss: 93.015589: 100%|██████████| 1600/1600 [00:05<00:00, 289.06it/s]\n","Train Epoch: 83/200 [78400/79999 (100%)]  Loss: 92.387251: 100%|██████████| 1600/1600 [00:05<00:00, 287.18it/s]\n","Train Epoch: 84/200 [78400/79999 (100%)]  Loss: 92.190797: 100%|██████████| 1600/1600 [00:05<00:00, 288.56it/s]\n","Train Epoch: 85/200 [78400/79999 (100%)]  Loss: 92.024963: 100%|██████████| 1600/1600 [00:05<00:00, 286.89it/s]\n","Train Epoch: 86/200 [78400/79999 (100%)]  Loss: 91.554010: 100%|██████████| 1600/1600 [00:05<00:00, 288.87it/s]\n","Train Epoch: 87/200 [78400/79999 (100%)]  Loss: 91.332776: 100%|██████████| 1600/1600 [00:05<00:00, 285.37it/s]\n","Train Epoch: 88/200 [78400/79999 (100%)]  Loss: 91.162444: 100%|██████████| 1600/1600 [00:05<00:00, 285.04it/s]\n","Train Epoch: 89/200 [78400/79999 (100%)]  Loss: 90.886000: 100%|██████████| 1600/1600 [00:05<00:00, 288.79it/s]\n","Train Epoch: 90/200 [78400/79999 (100%)]  Loss: 90.673531: 100%|██████████| 1600/1600 [00:05<00:00, 288.44it/s]\n","Train Epoch: 91/200 [78400/79999 (100%)]  Loss: 90.511137: 100%|██████████| 1600/1600 [00:05<00:00, 286.96it/s]\n","Train Epoch: 92/200 [78400/79999 (100%)]  Loss: 90.275959: 100%|██████████| 1600/1600 [00:05<00:00, 288.29it/s]\n","Train Epoch: 93/200 [78400/79999 (100%)]  Loss: 89.957723: 100%|██████████| 1600/1600 [00:05<00:00, 289.57it/s]\n","Train Epoch: 94/200 [78400/79999 (100%)]  Loss: 89.874456: 100%|██████████| 1600/1600 [00:05<00:00, 289.37it/s]\n","Train Epoch: 95/200 [78400/79999 (100%)]  Loss: 89.643052: 100%|██████████| 1600/1600 [00:05<00:00, 287.20it/s]\n","Train Epoch: 96/200 [78400/79999 (100%)]  Loss: 89.547164: 100%|██████████| 1600/1600 [00:05<00:00, 289.72it/s]\n","Train Epoch: 97/200 [78400/79999 (100%)]  Loss: 89.369292: 100%|██████████| 1600/1600 [00:05<00:00, 289.27it/s]\n","Train Epoch: 98/200 [78400/79999 (100%)]  Loss: 89.567784: 100%|██████████| 1600/1600 [00:05<00:00, 285.41it/s]\n","Train Epoch: 99/200 [78400/79999 (100%)]  Loss: 88.945630: 100%|██████████| 1600/1600 [00:05<00:00, 289.38it/s]\n","Train Epoch: 100/200 [78400/79999 (100%)]  Loss: 89.116021: 100%|██████████| 1600/1600 [00:05<00:00, 288.28it/s]\n","Train Epoch: 101/200 [78400/79999 (100%)]  Loss: 88.462292: 100%|██████████| 1600/1600 [00:05<00:00, 290.05it/s]\n","Train Epoch: 102/200 [78400/79999 (100%)]  Loss: 88.553976: 100%|██████████| 1600/1600 [00:05<00:00, 288.58it/s]\n","Train Epoch: 103/200 [78400/79999 (100%)]  Loss: 88.492588: 100%|██████████| 1600/1600 [00:05<00:00, 288.48it/s]\n","Train Epoch: 104/200 [78400/79999 (100%)]  Loss: 88.691363: 100%|██████████| 1600/1600 [00:05<00:00, 289.13it/s]\n","Train Epoch: 105/200 [78400/79999 (100%)]  Loss: 88.169849: 100%|██████████| 1600/1600 [00:05<00:00, 288.45it/s]\n","Train Epoch: 106/200 [78400/79999 (100%)]  Loss: 88.432883: 100%|██████████| 1600/1600 [00:05<00:00, 288.57it/s]\n","Train Epoch: 107/200 [78400/79999 (100%)]  Loss: 87.466696: 100%|██████████| 1600/1600 [00:05<00:00, 289.60it/s]\n","Train Epoch: 108/200 [78400/79999 (100%)]  Loss: 88.236104: 100%|██████████| 1600/1600 [00:05<00:00, 290.11it/s]\n","Train Epoch: 109/200 [78400/79999 (100%)]  Loss: 87.363476: 100%|██████████| 1600/1600 [00:05<00:00, 290.58it/s]\n","Train Epoch: 110/200 [78400/79999 (100%)]  Loss: 87.558262: 100%|██████████| 1600/1600 [00:05<00:00, 290.19it/s]\n","Train Epoch: 111/200 [78400/79999 (100%)]  Loss: 87.010386: 100%|██████████| 1600/1600 [00:05<00:00, 287.36it/s]\n","Train Epoch: 112/200 [78400/79999 (100%)]  Loss: 86.666950: 100%|██████████| 1600/1600 [00:05<00:00, 287.80it/s]\n","Train Epoch: 113/200 [78400/79999 (100%)]  Loss: 86.399134: 100%|██████████| 1600/1600 [00:05<00:00, 288.15it/s]\n","Train Epoch: 114/200 [78400/79999 (100%)]  Loss: 86.617622: 100%|██████████| 1600/1600 [00:05<00:00, 286.00it/s]\n","Train Epoch: 115/200 [78400/79999 (100%)]  Loss: 86.774574: 100%|██████████| 1600/1600 [00:05<00:00, 286.34it/s]\n","Train Epoch: 116/200 [78400/79999 (100%)]  Loss: 86.679453: 100%|██████████| 1600/1600 [00:05<00:00, 285.51it/s]\n","Train Epoch: 117/200 [78400/79999 (100%)]  Loss: 86.488637: 100%|██████████| 1600/1600 [00:05<00:00, 285.42it/s]\n","Train Epoch: 118/200 [78400/79999 (100%)]  Loss: 86.009447: 100%|██████████| 1600/1600 [00:05<00:00, 285.00it/s]\n","Train Epoch: 119/200 [78400/79999 (100%)]  Loss: 85.588810: 100%|██████████| 1600/1600 [00:05<00:00, 288.69it/s]\n","Train Epoch: 120/200 [78400/79999 (100%)]  Loss: 85.515307: 100%|██████████| 1600/1600 [00:05<00:00, 288.98it/s]\n","Train Epoch: 121/200 [78400/79999 (100%)]  Loss: 86.379742: 100%|██████████| 1600/1600 [00:05<00:00, 289.95it/s]\n","Train Epoch: 122/200 [78400/79999 (100%)]  Loss: 85.296852: 100%|██████████| 1600/1600 [00:05<00:00, 288.74it/s]\n","Train Epoch: 123/200 [78400/79999 (100%)]  Loss: 85.467559: 100%|██████████| 1600/1600 [00:05<00:00, 289.99it/s]\n","Train Epoch: 124/200 [78400/79999 (100%)]  Loss: 85.058162: 100%|██████████| 1600/1600 [00:05<00:00, 286.92it/s]\n","Train Epoch: 125/200 [78400/79999 (100%)]  Loss: 86.081836: 100%|██████████| 1600/1600 [00:05<00:00, 287.11it/s]\n","Train Epoch: 126/200 [78400/79999 (100%)]  Loss: 84.538615: 100%|██████████| 1600/1600 [00:05<00:00, 289.13it/s]\n","Train Epoch: 127/200 [78400/79999 (100%)]  Loss: 84.561603: 100%|██████████| 1600/1600 [00:05<00:00, 288.10it/s]\n","Train Epoch: 128/200 [78400/79999 (100%)]  Loss: 84.619819: 100%|██████████| 1600/1600 [00:05<00:00, 288.58it/s]\n","Train Epoch: 129/200 [78400/79999 (100%)]  Loss: 84.575074: 100%|██████████| 1600/1600 [00:05<00:00, 288.18it/s]\n","Train Epoch: 130/200 [78400/79999 (100%)]  Loss: 84.330195: 100%|██████████| 1600/1600 [00:05<00:00, 288.69it/s]\n","Train Epoch: 131/200 [78400/79999 (100%)]  Loss: 84.150470: 100%|██████████| 1600/1600 [00:05<00:00, 286.13it/s]\n","Train Epoch: 132/200 [78400/79999 (100%)]  Loss: 83.964281: 100%|██████████| 1600/1600 [00:05<00:00, 289.37it/s]\n","Train Epoch: 133/200 [78400/79999 (100%)]  Loss: 83.927645: 100%|██████████| 1600/1600 [00:05<00:00, 290.03it/s]\n","Train Epoch: 134/200 [78400/79999 (100%)]  Loss: 83.766820: 100%|██████████| 1600/1600 [00:05<00:00, 291.09it/s]\n","Train Epoch: 135/200 [78400/79999 (100%)]  Loss: 83.581520: 100%|██████████| 1600/1600 [00:05<00:00, 291.11it/s]\n","Train Epoch: 136/200 [78400/79999 (100%)]  Loss: 83.477499: 100%|██████████| 1600/1600 [00:05<00:00, 291.28it/s]\n","Train Epoch: 137/200 [78400/79999 (100%)]  Loss: 83.413273: 100%|██████████| 1600/1600 [00:05<00:00, 289.82it/s]\n","Train Epoch: 138/200 [78400/79999 (100%)]  Loss: 83.309181: 100%|██████████| 1600/1600 [00:05<00:00, 291.45it/s]\n","Train Epoch: 139/200 [78400/79999 (100%)]  Loss: 83.187509: 100%|██████████| 1600/1600 [00:05<00:00, 292.21it/s]\n","Train Epoch: 140/200 [78400/79999 (100%)]  Loss: 83.058564: 100%|██████████| 1600/1600 [00:05<00:00, 287.32it/s]\n","Train Epoch: 141/200 [78400/79999 (100%)]  Loss: 82.912489: 100%|██████████| 1600/1600 [00:05<00:00, 290.19it/s]\n","Train Epoch: 142/200 [78400/79999 (100%)]  Loss: 82.785735: 100%|██████████| 1600/1600 [00:05<00:00, 290.01it/s]\n","Train Epoch: 143/200 [78400/79999 (100%)]  Loss: 82.694496: 100%|██████████| 1600/1600 [00:05<00:00, 292.51it/s]\n","Train Epoch: 144/200 [78400/79999 (100%)]  Loss: 82.641578: 100%|██████████| 1600/1600 [00:05<00:00, 289.86it/s]\n","Train Epoch: 145/200 [78400/79999 (100%)]  Loss: 82.559044: 100%|██████████| 1600/1600 [00:05<00:00, 290.47it/s]\n","Train Epoch: 146/200 [78400/79999 (100%)]  Loss: 82.457081: 100%|██████████| 1600/1600 [00:05<00:00, 281.07it/s]\n","Train Epoch: 147/200 [78400/79999 (100%)]  Loss: 82.365862: 100%|██████████| 1600/1600 [00:05<00:00, 288.40it/s]\n","Train Epoch: 148/200 [78400/79999 (100%)]  Loss: 82.264329: 100%|██████████| 1600/1600 [00:05<00:00, 290.61it/s]\n","Train Epoch: 149/200 [78400/79999 (100%)]  Loss: 82.161323: 100%|██████████| 1600/1600 [00:05<00:00, 289.81it/s]\n","Train Epoch: 150/200 [78400/79999 (100%)]  Loss: 82.057308: 100%|██████████| 1600/1600 [00:05<00:00, 292.07it/s]\n","Train Epoch: 151/200 [78400/79999 (100%)]  Loss: 81.988286: 100%|██████████| 1600/1600 [00:05<00:00, 289.26it/s]\n","Train Epoch: 152/200 [78400/79999 (100%)]  Loss: 81.875001: 100%|██████████| 1600/1600 [00:05<00:00, 290.68it/s]\n","Train Epoch: 153/200 [78400/79999 (100%)]  Loss: 82.138269: 100%|██████████| 1600/1600 [00:05<00:00, 289.14it/s]\n","Train Epoch: 154/200 [78400/79999 (100%)]  Loss: 81.777674: 100%|██████████| 1600/1600 [00:05<00:00, 292.01it/s]\n","Train Epoch: 155/200 [78400/79999 (100%)]  Loss: 81.656980: 100%|██████████| 1600/1600 [00:05<00:00, 290.29it/s]\n","Train Epoch: 156/200 [78400/79999 (100%)]  Loss: 81.624050: 100%|██████████| 1600/1600 [00:05<00:00, 289.29it/s]\n","Train Epoch: 157/200 [78400/79999 (100%)]  Loss: 81.516291: 100%|██████████| 1600/1600 [00:05<00:00, 289.66it/s]\n","Train Epoch: 158/200 [78400/79999 (100%)]  Loss: 81.489931: 100%|██████████| 1600/1600 [00:05<00:00, 289.53it/s]\n","Train Epoch: 159/200 [78400/79999 (100%)]  Loss: 81.362015: 100%|██████████| 1600/1600 [00:05<00:00, 288.94it/s]\n","Train Epoch: 160/200 [78400/79999 (100%)]  Loss: 81.355249: 100%|██████████| 1600/1600 [00:05<00:00, 288.04it/s]\n","Train Epoch: 161/200 [78400/79999 (100%)]  Loss: 81.212305: 100%|██████████| 1600/1600 [00:05<00:00, 291.14it/s]\n","Train Epoch: 162/200 [78400/79999 (100%)]  Loss: 81.216796: 100%|██████████| 1600/1600 [00:05<00:00, 287.95it/s]\n","Train Epoch: 163/200 [78400/79999 (100%)]  Loss: 81.073427: 100%|██████████| 1600/1600 [00:05<00:00, 289.26it/s]\n","Train Epoch: 164/200 [78400/79999 (100%)]  Loss: 81.238862: 100%|██████████| 1600/1600 [00:05<00:00, 289.94it/s]\n","Train Epoch: 165/200 [78400/79999 (100%)]  Loss: 81.000366: 100%|██████████| 1600/1600 [00:05<00:00, 288.20it/s]\n","Train Epoch: 166/200 [78400/79999 (100%)]  Loss: 80.944968: 100%|██████████| 1600/1600 [00:05<00:00, 288.43it/s]\n","Train Epoch: 167/200 [78400/79999 (100%)]  Loss: 80.891884: 100%|██████████| 1600/1600 [00:05<00:00, 291.49it/s]\n","Train Epoch: 168/200 [78400/79999 (100%)]  Loss: 80.819538: 100%|██████████| 1600/1600 [00:05<00:00, 290.46it/s]\n","Train Epoch: 169/200 [78400/79999 (100%)]  Loss: 80.758920: 100%|██████████| 1600/1600 [00:05<00:00, 288.33it/s]\n","Train Epoch: 170/200 [78400/79999 (100%)]  Loss: 80.686531: 100%|██████████| 1600/1600 [00:05<00:00, 289.53it/s]\n","Train Epoch: 171/200 [78400/79999 (100%)]  Loss: 80.631665: 100%|██████████| 1600/1600 [00:05<00:00, 288.57it/s]\n","Train Epoch: 172/200 [78400/79999 (100%)]  Loss: 80.572181: 100%|██████████| 1600/1600 [00:05<00:00, 288.68it/s]\n","Train Epoch: 173/200 [78400/79999 (100%)]  Loss: 80.508177: 100%|██████████| 1600/1600 [00:05<00:00, 286.43it/s]\n","Train Epoch: 174/200 [78400/79999 (100%)]  Loss: 80.457533: 100%|██████████| 1600/1600 [00:05<00:00, 288.48it/s]\n","Train Epoch: 175/200 [78400/79999 (100%)]  Loss: 80.405889: 100%|██████████| 1600/1600 [00:05<00:00, 285.11it/s]\n","Train Epoch: 176/200 [78400/79999 (100%)]  Loss: 80.209117: 100%|██████████| 1600/1600 [00:05<00:00, 282.65it/s]\n","Train Epoch: 177/200 [78400/79999 (100%)]  Loss: 80.485667: 100%|██████████| 1600/1600 [00:05<00:00, 290.18it/s]\n","Train Epoch: 178/200 [78400/79999 (100%)]  Loss: 80.206186: 100%|██████████| 1600/1600 [00:05<00:00, 288.36it/s]\n","Train Epoch: 179/200 [78400/79999 (100%)]  Loss: 80.000500: 100%|██████████| 1600/1600 [00:05<00:00, 289.19it/s]\n","Train Epoch: 180/200 [78400/79999 (100%)]  Loss: 80.048440: 100%|██████████| 1600/1600 [00:05<00:00, 289.37it/s]\n","Train Epoch: 181/200 [78400/79999 (100%)]  Loss: 80.037669: 100%|██████████| 1600/1600 [00:05<00:00, 287.92it/s]\n","Train Epoch: 182/200 [78400/79999 (100%)]  Loss: 79.806320: 100%|██████████| 1600/1600 [00:05<00:00, 286.29it/s]\n","Train Epoch: 183/200 [78400/79999 (100%)]  Loss: 79.830640: 100%|██████████| 1600/1600 [00:05<00:00, 290.34it/s]\n","Train Epoch: 184/200 [78400/79999 (100%)]  Loss: 79.826497: 100%|██████████| 1600/1600 [00:05<00:00, 286.39it/s]\n","Train Epoch: 185/200 [78400/79999 (100%)]  Loss: 79.823960: 100%|██████████| 1600/1600 [00:05<00:00, 289.59it/s]\n","Train Epoch: 186/200 [78400/79999 (100%)]  Loss: 79.568798: 100%|██████████| 1600/1600 [00:05<00:00, 286.10it/s]\n","Train Epoch: 187/200 [78400/79999 (100%)]  Loss: 79.587548: 100%|██████████| 1600/1600 [00:05<00:00, 285.17it/s]\n","Train Epoch: 188/200 [78400/79999 (100%)]  Loss: 79.757416: 100%|██████████| 1600/1600 [00:05<00:00, 286.46it/s]\n","Train Epoch: 189/200 [78400/79999 (100%)]  Loss: 79.514967: 100%|██████████| 1600/1600 [00:05<00:00, 288.09it/s]\n","Train Epoch: 190/200 [78400/79999 (100%)]  Loss: 79.358234: 100%|██████████| 1600/1600 [00:05<00:00, 287.16it/s]\n","Train Epoch: 191/200 [78400/79999 (100%)]  Loss: 79.340597: 100%|██████████| 1600/1600 [00:05<00:00, 288.11it/s]\n","Train Epoch: 192/200 [78400/79999 (100%)]  Loss: 79.276908: 100%|██████████| 1600/1600 [00:05<00:00, 288.84it/s]\n","Train Epoch: 193/200 [78400/79999 (100%)]  Loss: 79.196022: 100%|██████████| 1600/1600 [00:05<00:00, 286.85it/s]\n","Train Epoch: 194/200 [78400/79999 (100%)]  Loss: 79.119869: 100%|██████████| 1600/1600 [00:05<00:00, 287.58it/s]\n","Train Epoch: 195/200 [78400/79999 (100%)]  Loss: 79.044248: 100%|██████████| 1600/1600 [00:05<00:00, 287.47it/s]\n","Train Epoch: 196/200 [78400/79999 (100%)]  Loss: 78.983144: 100%|██████████| 1600/1600 [00:05<00:00, 287.73it/s]\n","Train Epoch: 197/200 [78400/79999 (100%)]  Loss: 78.915493: 100%|██████████| 1600/1600 [00:05<00:00, 285.37it/s]\n","Train Epoch: 198/200 [78400/79999 (100%)]  Loss: 78.860729: 100%|██████████| 1600/1600 [00:05<00:00, 287.47it/s]\n","Train Epoch: 199/200 [78400/79999 (100%)]  Loss: 78.800047: 100%|██████████| 1600/1600 [00:05<00:00, 286.73it/s]\n","Train Epoch: 200/200 [78400/79999 (100%)]  Loss: 78.736176: 100%|██████████| 1600/1600 [00:05<00:00, 287.28it/s]\n"]}]},{"cell_type":"code","source":["\n","\n","start_time = time()\n","accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(noKD, criterion = nn.CrossEntropyLoss())\n","test_loss /= (split*sub)\n","\n","print('Result of testing noKD model')\n","print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n","print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%). Total time = {time() - start_time}')\n","print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%' .format(P, R, F1))"],"metadata":{"id":"iiF3Gp5ZJELK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665722897107,"user_tz":-420,"elapsed":1880,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"53ef09f6-9748-4b56-afea-68495e759cd2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Result of testing teacher model\n","false positive (FP): 1, false negative (FN): 504, true positive (TP): 2, true negative (TN): 19492\n","Test set: Average loss: 0.0022, Accuracy: 97.47%). Total time = 1.9545726776123047\n","Precision: 66.667%, Recall: 0.395%, F1-measure: 0.786%\n"]}]},{"cell_type":"code","source":["from torch.nn import Parameter\n","from torch.nn.modules.module import Module\n","import torch.nn.functional as F\n","import math\n","\n","save_student_path = ('../datasets/Spirit/model/chronological_student.pth')\n","def train_step(\n","    Teacher,\n","    Student,\n","    optimizer,\n","    student_loss_fn,\n","    divergence_loss_fn,\n","    temp,\n","    alpha,\n","    epoch,\n","    device\n","):\n","    losses = []\n","    pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch+1}\")\n","    for data, targets in pbar:\n","        # Get data to cuda if possible\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        # forward\n","        with torch.no_grad():\n","            teacher_preds, _ = Teacher(data)\n","\n","        student_preds, __ = Student(data)\n","        student_loss = student_loss_fn(student_preds, targets)\n","        \n","        ditillation_loss = divergence_loss_fn(\n","            F.softmax(student_preds / temp, dim=1),\n","            F.softmax(teacher_preds / temp, dim=1)\n","        )\n","        loss = alpha * student_loss + (1 - alpha) * ditillation_loss\n","        losses.append(loss.item())\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        optimizer.step()\n","    \n","    avg_loss = sum(losses) / len(losses)\n","    return avg_loss\n","\n","def teach(epochs, Teacher, Student, temp=7, alpha=0.3):\n","  Teacher = Teacher.to(device)\n","  Student = Student.to(device)\n","  student_loss_fn = nn.CrossEntropyLoss()\n","  divergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n","  optimizer = torch.optim.Adam(Student.parameters(), lr=0.001)\n","\n","  Teacher.eval()\n","  Student.train()\n","  for epoch in range(epochs):\n","      loss = train_step(\n","          Teacher,\n","          Student,\n","          optimizer,\n","          student_loss_fn,\n","          divergence_loss_fn,\n","          temp,\n","          alpha,\n","          epoch,\n","          device\n","      )\n","\n","      print(f\"Loss:{loss:.2f}\")\n","\n","Student = DistilLog(input_size = input_size, hidden_size=4, num_layers = 1, num_classes = num_classes, is_bidirectional=False).to(device)\n","\n","Teacher = load_model(Teacher, save_teacher_path)\n","teach(epochs=100, Teacher=Teacher, Student=Student, temp=7, alpha=0.3)\n","save_model(Student, save_student_path)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LBvMQ22vwonW","executionInfo":{"status":"ok","timestamp":1665723774698,"user_tz":-420,"elapsed":704079,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"8417e9dc-11eb-469e-b7c9-a0338e81db7a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 1600/1600 [00:07<00:00, 227.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.47\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 1600/1600 [00:07<00:00, 224.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.57\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 1600/1600 [00:06<00:00, 229.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.63\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 1600/1600 [00:07<00:00, 228.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.67\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 1600/1600 [00:06<00:00, 229.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.70\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 1600/1600 [00:06<00:00, 228.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.72\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|██████████| 1600/1600 [00:07<00:00, 228.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.73\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|██████████| 1600/1600 [00:07<00:00, 227.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.73\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|██████████| 1600/1600 [00:07<00:00, 225.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.74\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|██████████| 1600/1600 [00:07<00:00, 228.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.74\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11: 100%|██████████| 1600/1600 [00:07<00:00, 227.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.75\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12: 100%|██████████| 1600/1600 [00:06<00:00, 229.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.76\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13: 100%|██████████| 1600/1600 [00:07<00:00, 227.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.76\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14: 100%|██████████| 1600/1600 [00:06<00:00, 229.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.76\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15: 100%|██████████| 1600/1600 [00:07<00:00, 227.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.77\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16: 100%|██████████| 1600/1600 [00:06<00:00, 232.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.77\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17: 100%|██████████| 1600/1600 [00:07<00:00, 228.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.77\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18: 100%|██████████| 1600/1600 [00:07<00:00, 227.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.77\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19: 100%|██████████| 1600/1600 [00:07<00:00, 227.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.77\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20: 100%|██████████| 1600/1600 [00:07<00:00, 227.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.77\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21: 100%|██████████| 1600/1600 [00:07<00:00, 228.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22: 100%|██████████| 1600/1600 [00:07<00:00, 228.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23: 100%|██████████| 1600/1600 [00:07<00:00, 228.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 24: 100%|██████████| 1600/1600 [00:07<00:00, 228.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 25: 100%|██████████| 1600/1600 [00:07<00:00, 228.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26: 100%|██████████| 1600/1600 [00:07<00:00, 224.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 27: 100%|██████████| 1600/1600 [00:07<00:00, 227.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 28: 100%|██████████| 1600/1600 [00:07<00:00, 226.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 29: 100%|██████████| 1600/1600 [00:07<00:00, 227.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 30: 100%|██████████| 1600/1600 [00:07<00:00, 226.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 31: 100%|██████████| 1600/1600 [00:07<00:00, 226.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 32: 100%|██████████| 1600/1600 [00:07<00:00, 227.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 33: 100%|██████████| 1600/1600 [00:07<00:00, 228.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 34: 100%|██████████| 1600/1600 [00:07<00:00, 226.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 35: 100%|██████████| 1600/1600 [00:07<00:00, 227.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 36: 100%|██████████| 1600/1600 [00:07<00:00, 227.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 37: 100%|██████████| 1600/1600 [00:07<00:00, 227.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 38: 100%|██████████| 1600/1600 [00:07<00:00, 228.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.78\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 39: 100%|██████████| 1600/1600 [00:07<00:00, 227.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 40: 100%|██████████| 1600/1600 [00:07<00:00, 227.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 41: 100%|██████████| 1600/1600 [00:07<00:00, 228.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 42: 100%|██████████| 1600/1600 [00:07<00:00, 226.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 43: 100%|██████████| 1600/1600 [00:07<00:00, 227.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 44: 100%|██████████| 1600/1600 [00:07<00:00, 227.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 45: 100%|██████████| 1600/1600 [00:07<00:00, 225.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 46: 100%|██████████| 1600/1600 [00:07<00:00, 227.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 47: 100%|██████████| 1600/1600 [00:07<00:00, 228.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 48: 100%|██████████| 1600/1600 [00:06<00:00, 229.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 49: 100%|██████████| 1600/1600 [00:07<00:00, 224.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 50: 100%|██████████| 1600/1600 [00:06<00:00, 228.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 51: 100%|██████████| 1600/1600 [00:06<00:00, 228.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 52: 100%|██████████| 1600/1600 [00:07<00:00, 227.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 53: 100%|██████████| 1600/1600 [00:07<00:00, 226.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 54: 100%|██████████| 1600/1600 [00:07<00:00, 228.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 55: 100%|██████████| 1600/1600 [00:07<00:00, 228.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 56: 100%|██████████| 1600/1600 [00:06<00:00, 229.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 57: 100%|██████████| 1600/1600 [00:06<00:00, 229.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 58: 100%|██████████| 1600/1600 [00:06<00:00, 229.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 59: 100%|██████████| 1600/1600 [00:07<00:00, 228.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 60: 100%|██████████| 1600/1600 [00:07<00:00, 228.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 61: 100%|██████████| 1600/1600 [00:06<00:00, 228.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 62: 100%|██████████| 1600/1600 [00:07<00:00, 228.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 63: 100%|██████████| 1600/1600 [00:06<00:00, 229.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 64: 100%|██████████| 1600/1600 [00:06<00:00, 229.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 65: 100%|██████████| 1600/1600 [00:07<00:00, 228.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 66: 100%|██████████| 1600/1600 [00:07<00:00, 227.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 67: 100%|██████████| 1600/1600 [00:07<00:00, 227.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 68: 100%|██████████| 1600/1600 [00:07<00:00, 228.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 69: 100%|██████████| 1600/1600 [00:06<00:00, 230.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 70: 100%|██████████| 1600/1600 [00:07<00:00, 228.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 71: 100%|██████████| 1600/1600 [00:07<00:00, 227.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 72: 100%|██████████| 1600/1600 [00:07<00:00, 226.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.79\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 73: 100%|██████████| 1600/1600 [00:07<00:00, 222.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 74: 100%|██████████| 1600/1600 [00:07<00:00, 228.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 75: 100%|██████████| 1600/1600 [00:07<00:00, 228.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 76: 100%|██████████| 1600/1600 [00:07<00:00, 226.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 77: 100%|██████████| 1600/1600 [00:07<00:00, 228.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 78: 100%|██████████| 1600/1600 [00:06<00:00, 228.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 79: 100%|██████████| 1600/1600 [00:06<00:00, 229.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 80: 100%|██████████| 1600/1600 [00:07<00:00, 227.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 81: 100%|██████████| 1600/1600 [00:07<00:00, 228.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 82: 100%|██████████| 1600/1600 [00:07<00:00, 227.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 83: 100%|██████████| 1600/1600 [00:07<00:00, 227.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 84: 100%|██████████| 1600/1600 [00:07<00:00, 227.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 85: 100%|██████████| 1600/1600 [00:07<00:00, 227.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 86: 100%|██████████| 1600/1600 [00:07<00:00, 228.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 87: 100%|██████████| 1600/1600 [00:07<00:00, 227.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 88: 100%|██████████| 1600/1600 [00:07<00:00, 226.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 89: 100%|██████████| 1600/1600 [00:07<00:00, 227.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 90: 100%|██████████| 1600/1600 [00:07<00:00, 226.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 91: 100%|██████████| 1600/1600 [00:07<00:00, 228.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 92: 100%|██████████| 1600/1600 [00:07<00:00, 227.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 93: 100%|██████████| 1600/1600 [00:07<00:00, 227.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 94: 100%|██████████| 1600/1600 [00:07<00:00, 227.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 95: 100%|██████████| 1600/1600 [00:07<00:00, 228.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 96: 100%|██████████| 1600/1600 [00:07<00:00, 226.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 97: 100%|██████████| 1600/1600 [00:07<00:00, 222.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 98: 100%|██████████| 1600/1600 [00:07<00:00, 224.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 99: 100%|██████████| 1600/1600 [00:07<00:00, 225.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 100: 100%|██████████| 1600/1600 [00:07<00:00, 224.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.80\n"]}]},{"cell_type":"code","source":["start_time = time()\n","accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(Student, criterion = nn.CrossEntropyLoss())\n","test_loss /= (split*sub)\n","\n","print('Result of testing student model')\n","print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n","print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%). Total time = {time() - start_time}')\n","print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%' .format(P, R, F1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YaokKpIwxpv","executionInfo":{"status":"ok","timestamp":1665723776640,"user_tz":-420,"elapsed":1963,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"77b54073-7241-4b66-b354-a92cc22c8a3e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Result of testing student model\n","false positive (FP): 108, false negative (FN): 441, true positive (TP): 65, true negative (TN): 19385\n","Test set: Average loss: 0.0101, Accuracy: 97.25%). Total time = 1.957085132598877\n","Precision: 37.572%, Recall: 12.846%, F1-measure: 19.146%\n"]}]}]}