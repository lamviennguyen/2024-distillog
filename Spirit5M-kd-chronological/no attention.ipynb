{"cells":[{"cell_type":"code","source":["%cd ../"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxT_YOAMchne","executionInfo":{"status":"ok","timestamp":1668048731714,"user_tz":-420,"elapsed":817,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"3288c0b9-e4bf-4a59-e65d-e30286cf4468"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab/distillog\n"]}]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1668048618723,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"K9EQIe6D6ReS","outputId":"8f751ca0-285b-45c5-ab3e-beb7d0d94fb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/colab/distilllog/spirit-kd-chronological'\n","/content\n"]}],"source":["%cd drive/MyDrive/colab/distilllog/spirit-kd-chronological"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4711,"status":"ok","timestamp":1668048397269,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"2w366oWe31MW","outputId":"65837d38-f09b-4de3-816f-43d772e41c46"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.1)\n"]}],"source":["!pip install torchinfo"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"P3rXdIQ2NU8c","executionInfo":{"status":"ok","timestamp":1668048762173,"user_tz":-420,"elapsed":746,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.nn import Parameter\n","from torch.nn.modules.module import Module\n","from tqdm import tqdm\n","import torch.nn.utils.prune as prune\n","import torch.nn.functional as F\n","import math\n","import csv\n","from time import time \n","from torchinfo import summary\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"WPYeYB747GHn","executionInfo":{"status":"ok","timestamp":1668048762173,"user_tz":-420,"elapsed":14,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["num_classes = 2\n","batch_size = 50\n","learning_rate = 0.1\n","input_size = 30\n","sequence_length = 50\n","hidden_size = 128\n","num_layers = 2\n","split = 50\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_path = './datasets/Spirit/chronological_train.csv'\n","test_path = './datasets/Spirit/chronological_test.csv'\n","save_path = './datasets/Spirit/model/no-attention-teacher.pth'"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"L2JAIhBb5v4I","executionInfo":{"status":"ok","timestamp":1668048762174,"user_tz":-420,"elapsed":14,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["class TeacherModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","      super(TeacherModel, self).__init__()\n","      self.num_layers = num_layers\n","      self.hidden_size = hidden_size\n","      self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout = 0.1, batch_first=True)\n","      self.fc = nn.Linear(hidden_size, num_classes)\n","        \n","    def forward(self, x):\n","      out, _ = self.gru(x)\n","      out = out[:, -1, :]\n","      out = self.fc(out)\n","      return out\n","\n","class StudentModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","      super(StudentModel, self).__init__()\n","      self.num_layers = num_layers\n","      self.hidden_size = hidden_size\n","      self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout = 0.1, batch_first=True)\n","      self.fc = nn.Linear(hidden_size, num_classes)\n","        \n","    def forward(self, x):\n","      out, _ = self.gru(x)\n","      out = out[:, -1, :]\n","      out = self.fc(out)\n","      return out\n","\n","def load_model(model, save_path):\n","    model.load_state_dict(torch.load(save_path))\n","    return model\n","\n","def save_model(model, save_path):\n","    torch.save(model.state_dict(), save_path)\n","\n","def mod(l, n):\n","    \"\"\" Truncate or pad a list \"\"\"\n","    r = l[-1*n:]\n","    if len(r) < n:\n","        r.extend(list([0]) * (n - len(r)))\n","    return r\n","\n","def read_data(path, input_size, sequence_length):\n","    fi = pd.read_csv('./datasets/Spirit/pca_vector_newgenwithfasttext.csv', header = None)\n","    vec = []\n","    vec = fi\n","    vec = np.array(vec)\n","    \n","    logs_series = pd.read_csv(path)\n","    logs_series = logs_series.values\n","    label = logs_series[:,1]\n","    logs_data = logs_series[:,0]\n","    logs = []\n","    for i in range(0,len(logs_data)):\n","      ori_seq = [\n","          int(eventid) for eventid in logs_data[i].split()]\n","      seq_pattern = mod(ori_seq, sequence_length)\n","      vec_pattern = []\n","\n","      for event in seq_pattern:\n","        if event == 0:\n","          vec_pattern.append([-1]*input_size)\n","        else:\n","          vec_pattern.append(vec[event])  \n","      logs.append(vec_pattern)\n","    logs = np.array(logs)\n","    train_x = logs\n","    train_y = np.array(label)\n","    train_x = np.reshape(train_x, (train_x.shape[0], -1, input_size))\n","    train_y = train_y.astype(int)\n","\n","    return train_x, train_y\n","\n","def load_data(train_x, train_y, batch_size):\n","    tensor_x = torch.Tensor(train_x) \n","    tensor_y = torch.from_numpy(train_y)\n","    train_dataset = TensorDataset(tensor_x,tensor_y) \n","    train_loader = DataLoader(train_dataset, batch_size = batch_size) \n","    return train_loader\n"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"RrdDhd29ODtR","executionInfo":{"status":"ok","timestamp":1668048762174,"user_tz":-420,"elapsed":13,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["\n","def Teachertrain(model, train_loader, learning_rate, num_epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    #summary(model, input_size=(50, 50, 300))\n","    min_loss = 100\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 0.0001)  \n","    model.train()\n","    \n","    for epoch in range(num_epochs):\n","        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n","        total_loss = 0\n","        for batch_idx, (data, target) in pbar:\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            total_loss += loss.item() \n","            loss.backward()          \n","            optimizer.step()\n","            if total_loss < min_loss:\n","                min_loss = total_loss\n","                save_model(Teacher,'./datasets/Spirit/model/no-attention-teacher.pth')\n","\n","            if (batch_idx+1) % 10 == 0:\n","                done = (batch_idx+1) * len(data)\n","                percentage = 100. * batch_idx / len(train_loader)\n","                pbar.set_description(f'Train Epoch: {epoch+1}/{num_epochs} [{done:5}/{len(train_loader.dataset)} ({percentage:3.0f}%)]  Loss: {total_loss:.6f}')\n","    return model"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"8LtzGZcK6Qdx","executionInfo":{"status":"ok","timestamp":1668048762721,"user_tz":-420,"elapsed":559,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["fi = pd.read_csv('./datasets/Spirit/pca_vector_newgenwithfasttext.csv', header = None)\n","vec = []\n","vec = fi\n","vec = np.array(vec)\n","\n","test_logs_series = pd.read_csv(test_path)\n","test_logs_series = test_logs_series.values\n","test_total = len(test_logs_series)\n","sub = int(test_total/split)\n","\n","\n","def load_test(i):\n","    if i!=split-1:\n","        label = test_logs_series[i*sub:(i+1)*sub,1]\n","        logs_data = test_logs_series[i*sub:(i+1)*sub,0]\n","    else:\n","        label = test_logs_series[i*sub:,1]\n","        logs_data = test_logs_series[i*sub:,0]\n","    logs = []\n","\n","    for logid in range(0,len(logs_data)):\n","        ori_seq = [\n","            int(eventid) for eventid in logs_data[logid].split()]\n","        seq_pattern = mod(ori_seq, sequence_length)\n","        vec_pattern = []\n","\n","        for event in seq_pattern:\n","            if event == 0:\n","                vec_pattern.append([-1]*input_size)\n","            else:\n","                vec_pattern.append(vec[event])  \n","        logs.append(vec_pattern)\n","    logs = np.array(logs)\n","    train_x = logs\n","    train_y = label\n","    train_x = np.reshape(train_x, (train_x.shape[0], -1, input_size))\n","    train_y = train_y.astype(int)\n","    return train_x, train_y\n","\n","\n","\n","def test(model, criterion = nn.CrossEntropyLoss()):\n","    model.eval()\n","    test_loss = 0\n","    with torch.no_grad():\n","        TP = 0 \n","        FP = 0\n","        FN = 0 \n","        TN = 0\n","        for i in range (0, split):        #################################################\n","            #print(f'batch {i+1}/{split}')\n","            test_x, test_y = load_test(i)\n","            test_loader = load_data(test_x, test_y, batch_size)            \n","            for data, target in test_loader:\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                test_loss += criterion(output, target) # sum up batch loss\n","                \n","                output = torch.sigmoid(output)[:, 0].cpu().detach().numpy()\n","                predicted = (output < 0.2).astype(int)\n","                target = np.array([y.cpu() for y in target])\n","                #print(predicted, label)\n","                TP += ((predicted == 1) * (target == 1)).sum()\n","                FP += ((predicted == 1) * (target == 0)).sum()\n","                FN += ((predicted == 0) * (target == 1)).sum()\n","                TN += ((predicted == 0) * (target == 0)).sum()\n","        P = 100 * TP / (TP + FP)\n","        R = 100 * TP / (TP + FN)\n","        F1 = 2 * P * R / (P + R)   \n","        accuracy = 100 * (TP + TN)/(TP + TN + FP + FN)  \n","        #MCC =       \n","    return accuracy, test_loss, P, R, F1, TP, FP, TN, FN"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1668048762721,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"B0iY8NxI63j0","outputId":"501a8600-b7da-41f0-e1d0-6003bc89a1f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","TeacherModel                             [50, 2]                   --\n","├─GRU: 1-1                               [50, 50, 128]             160,512\n","├─Linear: 1-2                            [50, 2]                   258\n","==========================================================================================\n","Total params: 160,770\n","Trainable params: 160,770\n","Non-trainable params: 0\n","Total mult-adds (M): 401.29\n","==========================================================================================\n","Input size (MB): 0.30\n","Forward/backward pass size (MB): 2.56\n","Params size (MB): 0.64\n","Estimated Total Size (MB): 3.50\n","=========================================================================================="]},"metadata":{},"execution_count":55}],"source":["Teacher = TeacherModel(input_size, hidden_size, num_layers, num_classes).to(device)\n","summary(Teacher, input_size=(batch_size, sequence_length, input_size))"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"eGooFjpq93Dq","executionInfo":{"status":"ok","timestamp":1668048762722,"user_tz":-420,"elapsed":6,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["Student = StudentModel(input_size = 30, hidden_size = 6, num_layers = 2, num_classes = 2).to(device)\n","#summary(Student, input_size=(batch_size, sequence_length, input_size))"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"iYOEZPRxCG75","executionInfo":{"status":"ok","timestamp":1668048768741,"user_tz":-420,"elapsed":6024,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}}},"outputs":[],"source":["train_x, train_y = read_data(train_path, input_size, sequence_length)\n","train_loader = load_data(train_x, train_y, batch_size)"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"UkcDP_0WDlod","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668050083807,"user_tz":-420,"elapsed":1304538,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"90a0f02b-a493-4387-aa35-249d3d34f2f8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Train Epoch: 1/200 [78400/79999 (100%)]  Loss: 375.610310: 100%|██████████| 1600/1600 [00:06<00:00, 229.73it/s]\n","Train Epoch: 2/200 [78400/79999 (100%)]  Loss: 164.162434: 100%|██████████| 1600/1600 [00:06<00:00, 252.29it/s]\n","Train Epoch: 3/200 [78400/79999 (100%)]  Loss: 73.779552: 100%|██████████| 1600/1600 [00:07<00:00, 227.81it/s]\n","Train Epoch: 4/200 [78400/79999 (100%)]  Loss: 72.683173: 100%|██████████| 1600/1600 [00:06<00:00, 250.50it/s]\n","Train Epoch: 5/200 [78400/79999 (100%)]  Loss: 65.356550: 100%|██████████| 1600/1600 [00:06<00:00, 250.78it/s]\n","Train Epoch: 6/200 [78400/79999 (100%)]  Loss: 61.677373: 100%|██████████| 1600/1600 [00:06<00:00, 249.22it/s]\n","Train Epoch: 7/200 [78400/79999 (100%)]  Loss: 59.327859: 100%|██████████| 1600/1600 [00:06<00:00, 251.97it/s]\n","Train Epoch: 8/200 [78400/79999 (100%)]  Loss: 56.757760: 100%|██████████| 1600/1600 [00:06<00:00, 248.19it/s]\n","Train Epoch: 9/200 [78400/79999 (100%)]  Loss: 55.047298: 100%|██████████| 1600/1600 [00:06<00:00, 250.09it/s]\n","Train Epoch: 10/200 [78400/79999 (100%)]  Loss: 53.929522: 100%|██████████| 1600/1600 [00:06<00:00, 248.67it/s]\n","Train Epoch: 11/200 [78400/79999 (100%)]  Loss: 51.757831: 100%|██████████| 1600/1600 [00:06<00:00, 247.66it/s]\n","Train Epoch: 12/200 [78400/79999 (100%)]  Loss: 50.664490: 100%|██████████| 1600/1600 [00:06<00:00, 250.77it/s]\n","Train Epoch: 13/200 [78400/79999 (100%)]  Loss: 44.497454: 100%|██████████| 1600/1600 [00:06<00:00, 253.59it/s]\n","Train Epoch: 14/200 [78400/79999 (100%)]  Loss: 40.472740: 100%|██████████| 1600/1600 [00:06<00:00, 251.66it/s]\n","Train Epoch: 15/200 [78400/79999 (100%)]  Loss: 37.644904: 100%|██████████| 1600/1600 [00:06<00:00, 250.55it/s]\n","Train Epoch: 16/200 [78400/79999 (100%)]  Loss: 35.693506: 100%|██████████| 1600/1600 [00:06<00:00, 251.74it/s]\n","Train Epoch: 17/200 [78400/79999 (100%)]  Loss: 29.944207: 100%|██████████| 1600/1600 [00:06<00:00, 250.96it/s]\n","Train Epoch: 18/200 [78400/79999 (100%)]  Loss: 27.826178: 100%|██████████| 1600/1600 [00:06<00:00, 251.38it/s]\n","Train Epoch: 19/200 [78400/79999 (100%)]  Loss: 25.450247: 100%|██████████| 1600/1600 [00:06<00:00, 251.21it/s]\n","Train Epoch: 20/200 [78400/79999 (100%)]  Loss: 25.692918: 100%|██████████| 1600/1600 [00:06<00:00, 248.04it/s]\n","Train Epoch: 21/200 [78400/79999 (100%)]  Loss: 23.006785: 100%|██████████| 1600/1600 [00:06<00:00, 250.63it/s]\n","Train Epoch: 22/200 [78400/79999 (100%)]  Loss: 22.333032: 100%|██████████| 1600/1600 [00:07<00:00, 220.32it/s]\n","Train Epoch: 23/200 [78400/79999 (100%)]  Loss: 21.410899: 100%|██████████| 1600/1600 [00:06<00:00, 248.45it/s]\n","Train Epoch: 24/200 [78400/79999 (100%)]  Loss: 20.197405: 100%|██████████| 1600/1600 [00:06<00:00, 239.84it/s]\n","Train Epoch: 25/200 [78400/79999 (100%)]  Loss: 19.249979: 100%|██████████| 1600/1600 [00:06<00:00, 249.76it/s]\n","Train Epoch: 26/200 [78400/79999 (100%)]  Loss: 19.309624: 100%|██████████| 1600/1600 [00:06<00:00, 251.18it/s]\n","Train Epoch: 27/200 [78400/79999 (100%)]  Loss: 19.908536: 100%|██████████| 1600/1600 [00:06<00:00, 250.65it/s]\n","Train Epoch: 28/200 [78400/79999 (100%)]  Loss: 21.412405: 100%|██████████| 1600/1600 [00:06<00:00, 250.91it/s]\n","Train Epoch: 29/200 [78400/79999 (100%)]  Loss: 19.933038: 100%|██████████| 1600/1600 [00:06<00:00, 250.24it/s]\n","Train Epoch: 30/200 [78400/79999 (100%)]  Loss: 20.157555: 100%|██████████| 1600/1600 [00:06<00:00, 251.22it/s]\n","Train Epoch: 31/200 [78400/79999 (100%)]  Loss: 18.929753: 100%|██████████| 1600/1600 [00:06<00:00, 251.87it/s]\n","Train Epoch: 32/200 [78400/79999 (100%)]  Loss: 20.210156: 100%|██████████| 1600/1600 [00:06<00:00, 251.06it/s]\n","Train Epoch: 33/200 [78400/79999 (100%)]  Loss: 18.778947: 100%|██████████| 1600/1600 [00:06<00:00, 249.43it/s]\n","Train Epoch: 34/200 [78400/79999 (100%)]  Loss: 17.840516: 100%|██████████| 1600/1600 [00:06<00:00, 247.55it/s]\n","Train Epoch: 35/200 [78400/79999 (100%)]  Loss: 17.830812: 100%|██████████| 1600/1600 [00:06<00:00, 250.77it/s]\n","Train Epoch: 36/200 [78400/79999 (100%)]  Loss: 17.423506: 100%|██████████| 1600/1600 [00:06<00:00, 248.94it/s]\n","Train Epoch: 37/200 [78400/79999 (100%)]  Loss: 17.879758: 100%|██████████| 1600/1600 [00:06<00:00, 250.78it/s]\n","Train Epoch: 38/200 [78400/79999 (100%)]  Loss: 17.969771: 100%|██████████| 1600/1600 [00:06<00:00, 250.76it/s]\n","Train Epoch: 39/200 [78400/79999 (100%)]  Loss: 19.583668: 100%|██████████| 1600/1600 [00:06<00:00, 249.00it/s]\n","Train Epoch: 40/200 [78400/79999 (100%)]  Loss: 17.575269: 100%|██████████| 1600/1600 [00:06<00:00, 251.61it/s]\n","Train Epoch: 41/200 [78400/79999 (100%)]  Loss: 17.323871: 100%|██████████| 1600/1600 [00:06<00:00, 249.67it/s]\n","Train Epoch: 42/200 [78400/79999 (100%)]  Loss: 17.616370: 100%|██████████| 1600/1600 [00:06<00:00, 250.14it/s]\n","Train Epoch: 43/200 [78400/79999 (100%)]  Loss: 16.909859: 100%|██████████| 1600/1600 [00:06<00:00, 248.00it/s]\n","Train Epoch: 44/200 [78400/79999 (100%)]  Loss: 17.822919: 100%|██████████| 1600/1600 [00:06<00:00, 251.35it/s]\n","Train Epoch: 45/200 [78400/79999 (100%)]  Loss: 17.860597: 100%|██████████| 1600/1600 [00:06<00:00, 250.74it/s]\n","Train Epoch: 46/200 [78400/79999 (100%)]  Loss: 16.652638: 100%|██████████| 1600/1600 [00:06<00:00, 250.07it/s]\n","Train Epoch: 47/200 [78400/79999 (100%)]  Loss: 16.836416: 100%|██████████| 1600/1600 [00:06<00:00, 248.51it/s]\n","Train Epoch: 48/200 [78400/79999 (100%)]  Loss: 17.822650: 100%|██████████| 1600/1600 [00:06<00:00, 248.65it/s]\n","Train Epoch: 49/200 [78400/79999 (100%)]  Loss: 16.938199: 100%|██████████| 1600/1600 [00:06<00:00, 252.08it/s]\n","Train Epoch: 50/200 [78400/79999 (100%)]  Loss: 17.164775: 100%|██████████| 1600/1600 [00:06<00:00, 246.13it/s]\n","Train Epoch: 51/200 [78400/79999 (100%)]  Loss: 16.949699: 100%|██████████| 1600/1600 [00:06<00:00, 249.93it/s]\n","Train Epoch: 52/200 [78400/79999 (100%)]  Loss: 16.873589: 100%|██████████| 1600/1600 [00:06<00:00, 251.80it/s]\n","Train Epoch: 53/200 [78400/79999 (100%)]  Loss: 16.491554: 100%|██████████| 1600/1600 [00:06<00:00, 234.21it/s]\n","Train Epoch: 54/200 [78400/79999 (100%)]  Loss: 16.753530: 100%|██████████| 1600/1600 [00:06<00:00, 235.55it/s]\n","Train Epoch: 55/200 [78400/79999 (100%)]  Loss: 16.744724: 100%|██████████| 1600/1600 [00:06<00:00, 249.44it/s]\n","Train Epoch: 56/200 [78400/79999 (100%)]  Loss: 16.912448: 100%|██████████| 1600/1600 [00:06<00:00, 250.81it/s]\n","Train Epoch: 57/200 [78400/79999 (100%)]  Loss: 17.239494: 100%|██████████| 1600/1600 [00:06<00:00, 249.77it/s]\n","Train Epoch: 58/200 [78400/79999 (100%)]  Loss: 16.863419: 100%|██████████| 1600/1600 [00:06<00:00, 247.67it/s]\n","Train Epoch: 59/200 [78400/79999 (100%)]  Loss: 16.749317: 100%|██████████| 1600/1600 [00:06<00:00, 242.05it/s]\n","Train Epoch: 60/200 [78400/79999 (100%)]  Loss: 16.129303: 100%|██████████| 1600/1600 [00:06<00:00, 246.71it/s]\n","Train Epoch: 61/200 [78400/79999 (100%)]  Loss: 15.698170: 100%|██████████| 1600/1600 [00:06<00:00, 250.34it/s]\n","Train Epoch: 62/200 [78400/79999 (100%)]  Loss: 16.058258: 100%|██████████| 1600/1600 [00:06<00:00, 249.49it/s]\n","Train Epoch: 63/200 [78400/79999 (100%)]  Loss: 16.692643: 100%|██████████| 1600/1600 [00:06<00:00, 248.45it/s]\n","Train Epoch: 64/200 [78400/79999 (100%)]  Loss: 16.112837: 100%|██████████| 1600/1600 [00:06<00:00, 248.13it/s]\n","Train Epoch: 65/200 [78400/79999 (100%)]  Loss: 16.581554: 100%|██████████| 1600/1600 [00:06<00:00, 250.11it/s]\n","Train Epoch: 66/200 [78400/79999 (100%)]  Loss: 15.817098: 100%|██████████| 1600/1600 [00:06<00:00, 248.13it/s]\n","Train Epoch: 67/200 [78400/79999 (100%)]  Loss: 15.459485: 100%|██████████| 1600/1600 [00:06<00:00, 251.04it/s]\n","Train Epoch: 68/200 [78400/79999 (100%)]  Loss: 16.326824: 100%|██████████| 1600/1600 [00:06<00:00, 251.98it/s]\n","Train Epoch: 69/200 [78400/79999 (100%)]  Loss: 16.250891: 100%|██████████| 1600/1600 [00:06<00:00, 247.23it/s]\n","Train Epoch: 70/200 [78400/79999 (100%)]  Loss: 15.483306: 100%|██████████| 1600/1600 [00:06<00:00, 249.31it/s]\n","Train Epoch: 71/200 [78400/79999 (100%)]  Loss: 15.578270: 100%|██████████| 1600/1600 [00:06<00:00, 246.58it/s]\n","Train Epoch: 72/200 [78400/79999 (100%)]  Loss: 15.640481: 100%|██████████| 1600/1600 [00:06<00:00, 248.23it/s]\n","Train Epoch: 73/200 [78400/79999 (100%)]  Loss: 15.307730: 100%|██████████| 1600/1600 [00:06<00:00, 249.56it/s]\n","Train Epoch: 74/200 [78400/79999 (100%)]  Loss: 15.588349: 100%|██████████| 1600/1600 [00:06<00:00, 248.44it/s]\n","Train Epoch: 75/200 [78400/79999 (100%)]  Loss: 16.049923: 100%|██████████| 1600/1600 [00:06<00:00, 246.62it/s]\n","Train Epoch: 76/200 [78400/79999 (100%)]  Loss: 14.894559: 100%|██████████| 1600/1600 [00:06<00:00, 248.35it/s]\n","Train Epoch: 77/200 [78400/79999 (100%)]  Loss: 15.201573: 100%|██████████| 1600/1600 [00:06<00:00, 249.19it/s]\n","Train Epoch: 78/200 [78400/79999 (100%)]  Loss: 15.788430: 100%|██████████| 1600/1600 [00:06<00:00, 248.16it/s]\n","Train Epoch: 79/200 [78400/79999 (100%)]  Loss: 14.931347: 100%|██████████| 1600/1600 [00:06<00:00, 246.81it/s]\n","Train Epoch: 80/200 [78400/79999 (100%)]  Loss: 14.875886: 100%|██████████| 1600/1600 [00:06<00:00, 248.26it/s]\n","Train Epoch: 81/200 [78400/79999 (100%)]  Loss: 15.030624: 100%|██████████| 1600/1600 [00:06<00:00, 246.52it/s]\n","Train Epoch: 82/200 [78400/79999 (100%)]  Loss: 14.755106: 100%|██████████| 1600/1600 [00:06<00:00, 248.30it/s]\n","Train Epoch: 83/200 [78400/79999 (100%)]  Loss: 15.675195: 100%|██████████| 1600/1600 [00:06<00:00, 248.45it/s]\n","Train Epoch: 84/200 [78400/79999 (100%)]  Loss: 14.649856: 100%|██████████| 1600/1600 [00:06<00:00, 245.45it/s]\n","Train Epoch: 85/200 [78400/79999 (100%)]  Loss: 14.902676: 100%|██████████| 1600/1600 [00:07<00:00, 218.99it/s]\n","Train Epoch: 86/200 [78400/79999 (100%)]  Loss: 14.584657: 100%|██████████| 1600/1600 [00:06<00:00, 248.72it/s]\n","Train Epoch: 87/200 [78400/79999 (100%)]  Loss: 15.285562: 100%|██████████| 1600/1600 [00:06<00:00, 249.48it/s]\n","Train Epoch: 88/200 [78400/79999 (100%)]  Loss: 14.459934: 100%|██████████| 1600/1600 [00:06<00:00, 247.16it/s]\n","Train Epoch: 89/200 [78400/79999 (100%)]  Loss: 14.937048: 100%|██████████| 1600/1600 [00:06<00:00, 248.69it/s]\n","Train Epoch: 90/200 [78400/79999 (100%)]  Loss: 14.273005: 100%|██████████| 1600/1600 [00:06<00:00, 247.36it/s]\n","Train Epoch: 91/200 [78400/79999 (100%)]  Loss: 14.408592: 100%|██████████| 1600/1600 [00:06<00:00, 247.84it/s]\n","Train Epoch: 92/200 [78400/79999 (100%)]  Loss: 14.441894: 100%|██████████| 1600/1600 [00:06<00:00, 248.63it/s]\n","Train Epoch: 93/200 [78400/79999 (100%)]  Loss: 14.260547: 100%|██████████| 1600/1600 [00:06<00:00, 247.77it/s]\n","Train Epoch: 94/200 [78400/79999 (100%)]  Loss: 14.243828: 100%|██████████| 1600/1600 [00:06<00:00, 246.55it/s]\n","Train Epoch: 95/200 [78400/79999 (100%)]  Loss: 14.497697: 100%|██████████| 1600/1600 [00:06<00:00, 248.40it/s]\n","Train Epoch: 96/200 [78400/79999 (100%)]  Loss: 14.547627: 100%|██████████| 1600/1600 [00:06<00:00, 247.69it/s]\n","Train Epoch: 97/200 [78400/79999 (100%)]  Loss: 14.743438: 100%|██████████| 1600/1600 [00:06<00:00, 246.37it/s]\n","Train Epoch: 98/200 [78400/79999 (100%)]  Loss: 13.658361: 100%|██████████| 1600/1600 [00:06<00:00, 247.41it/s]\n","Train Epoch: 99/200 [78400/79999 (100%)]  Loss: 14.286304: 100%|██████████| 1600/1600 [00:06<00:00, 248.18it/s]\n","Train Epoch: 100/200 [78400/79999 (100%)]  Loss: 14.814573: 100%|██████████| 1600/1600 [00:06<00:00, 248.09it/s]\n","Train Epoch: 101/200 [78400/79999 (100%)]  Loss: 13.740227: 100%|██████████| 1600/1600 [00:06<00:00, 250.45it/s]\n","Train Epoch: 102/200 [78400/79999 (100%)]  Loss: 13.832128: 100%|██████████| 1600/1600 [00:06<00:00, 247.71it/s]\n","Train Epoch: 103/200 [78400/79999 (100%)]  Loss: 14.155580: 100%|██████████| 1600/1600 [00:06<00:00, 247.68it/s]\n","Train Epoch: 104/200 [78400/79999 (100%)]  Loss: 14.228287: 100%|██████████| 1600/1600 [00:06<00:00, 247.29it/s]\n","Train Epoch: 105/200 [78400/79999 (100%)]  Loss: 13.590666: 100%|██████████| 1600/1600 [00:06<00:00, 247.54it/s]\n","Train Epoch: 106/200 [78400/79999 (100%)]  Loss: 14.153507: 100%|██████████| 1600/1600 [00:06<00:00, 246.96it/s]\n","Train Epoch: 107/200 [78400/79999 (100%)]  Loss: 14.420767: 100%|██████████| 1600/1600 [00:06<00:00, 241.68it/s]\n","Train Epoch: 108/200 [78400/79999 (100%)]  Loss: 13.933554: 100%|██████████| 1600/1600 [00:06<00:00, 246.64it/s]\n","Train Epoch: 109/200 [78400/79999 (100%)]  Loss: 13.894403: 100%|██████████| 1600/1600 [00:06<00:00, 244.32it/s]\n","Train Epoch: 110/200 [78400/79999 (100%)]  Loss: 14.051072: 100%|██████████| 1600/1600 [00:06<00:00, 247.41it/s]\n","Train Epoch: 111/200 [78400/79999 (100%)]  Loss: 13.759147: 100%|██████████| 1600/1600 [00:06<00:00, 249.25it/s]\n","Train Epoch: 112/200 [78400/79999 (100%)]  Loss: 13.640859: 100%|██████████| 1600/1600 [00:06<00:00, 244.85it/s]\n","Train Epoch: 113/200 [78400/79999 (100%)]  Loss: 13.836130: 100%|██████████| 1600/1600 [00:06<00:00, 245.39it/s]\n","Train Epoch: 114/200 [78400/79999 (100%)]  Loss: 13.663186: 100%|██████████| 1600/1600 [00:06<00:00, 246.49it/s]\n","Train Epoch: 115/200 [78400/79999 (100%)]  Loss: 14.083676: 100%|██████████| 1600/1600 [00:06<00:00, 246.21it/s]\n","Train Epoch: 116/200 [78400/79999 (100%)]  Loss: 13.844884: 100%|██████████| 1600/1600 [00:07<00:00, 216.68it/s]\n","Train Epoch: 117/200 [78400/79999 (100%)]  Loss: 13.624496: 100%|██████████| 1600/1600 [00:06<00:00, 248.00it/s]\n","Train Epoch: 118/200 [78400/79999 (100%)]  Loss: 13.876493: 100%|██████████| 1600/1600 [00:06<00:00, 246.07it/s]\n","Train Epoch: 119/200 [78400/79999 (100%)]  Loss: 13.594527: 100%|██████████| 1600/1600 [00:06<00:00, 245.79it/s]\n","Train Epoch: 120/200 [78400/79999 (100%)]  Loss: 13.415199: 100%|██████████| 1600/1600 [00:06<00:00, 248.53it/s]\n","Train Epoch: 121/200 [78400/79999 (100%)]  Loss: 13.255330: 100%|██████████| 1600/1600 [00:06<00:00, 246.42it/s]\n","Train Epoch: 122/200 [78400/79999 (100%)]  Loss: 13.736879: 100%|██████████| 1600/1600 [00:06<00:00, 243.86it/s]\n","Train Epoch: 123/200 [78400/79999 (100%)]  Loss: 13.833955: 100%|██████████| 1600/1600 [00:06<00:00, 245.20it/s]\n","Train Epoch: 124/200 [78400/79999 (100%)]  Loss: 14.307424: 100%|██████████| 1600/1600 [00:06<00:00, 247.41it/s]\n","Train Epoch: 125/200 [78400/79999 (100%)]  Loss: 13.511642: 100%|██████████| 1600/1600 [00:06<00:00, 245.30it/s]\n","Train Epoch: 126/200 [78400/79999 (100%)]  Loss: 13.113885: 100%|██████████| 1600/1600 [00:06<00:00, 247.54it/s]\n","Train Epoch: 127/200 [78400/79999 (100%)]  Loss: 12.938742: 100%|██████████| 1600/1600 [00:06<00:00, 245.98it/s]\n","Train Epoch: 128/200 [78400/79999 (100%)]  Loss: 13.687878: 100%|██████████| 1600/1600 [00:06<00:00, 248.75it/s]\n","Train Epoch: 129/200 [78400/79999 (100%)]  Loss: 12.606681: 100%|██████████| 1600/1600 [00:06<00:00, 246.15it/s]\n","Train Epoch: 130/200 [78400/79999 (100%)]  Loss: 13.316816: 100%|██████████| 1600/1600 [00:06<00:00, 240.15it/s]\n","Train Epoch: 131/200 [78400/79999 (100%)]  Loss: 13.103296: 100%|██████████| 1600/1600 [00:06<00:00, 241.89it/s]\n","Train Epoch: 132/200 [78400/79999 (100%)]  Loss: 13.401019: 100%|██████████| 1600/1600 [00:06<00:00, 245.42it/s]\n","Train Epoch: 133/200 [78400/79999 (100%)]  Loss: 12.854933: 100%|██████████| 1600/1600 [00:06<00:00, 244.51it/s]\n","Train Epoch: 134/200 [78400/79999 (100%)]  Loss: 13.179070: 100%|██████████| 1600/1600 [00:06<00:00, 244.43it/s]\n","Train Epoch: 135/200 [78400/79999 (100%)]  Loss: 13.203636: 100%|██████████| 1600/1600 [00:06<00:00, 243.13it/s]\n","Train Epoch: 136/200 [78400/79999 (100%)]  Loss: 12.990603: 100%|██████████| 1600/1600 [00:06<00:00, 247.10it/s]\n","Train Epoch: 137/200 [78400/79999 (100%)]  Loss: 12.502650: 100%|██████████| 1600/1600 [00:06<00:00, 245.72it/s]\n","Train Epoch: 138/200 [78400/79999 (100%)]  Loss: 12.528402: 100%|██████████| 1600/1600 [00:06<00:00, 247.14it/s]\n","Train Epoch: 139/200 [78400/79999 (100%)]  Loss: 12.844788: 100%|██████████| 1600/1600 [00:06<00:00, 244.54it/s]\n","Train Epoch: 140/200 [78400/79999 (100%)]  Loss: 12.128971: 100%|██████████| 1600/1600 [00:06<00:00, 243.71it/s]\n","Train Epoch: 141/200 [78400/79999 (100%)]  Loss: 12.964983: 100%|██████████| 1600/1600 [00:06<00:00, 242.86it/s]\n","Train Epoch: 142/200 [78400/79999 (100%)]  Loss: 12.440656: 100%|██████████| 1600/1600 [00:06<00:00, 247.70it/s]\n","Train Epoch: 143/200 [78400/79999 (100%)]  Loss: 12.120521: 100%|██████████| 1600/1600 [00:06<00:00, 247.73it/s]\n","Train Epoch: 144/200 [78400/79999 (100%)]  Loss: 12.487874: 100%|██████████| 1600/1600 [00:06<00:00, 243.80it/s]\n","Train Epoch: 145/200 [78400/79999 (100%)]  Loss: 12.734588: 100%|██████████| 1600/1600 [00:06<00:00, 245.37it/s]\n","Train Epoch: 146/200 [78400/79999 (100%)]  Loss: 12.071675: 100%|██████████| 1600/1600 [00:06<00:00, 241.59it/s]\n","Train Epoch: 147/200 [78400/79999 (100%)]  Loss: 12.372962: 100%|██████████| 1600/1600 [00:07<00:00, 216.31it/s]\n","Train Epoch: 148/200 [78400/79999 (100%)]  Loss: 11.999064: 100%|██████████| 1600/1600 [00:06<00:00, 244.58it/s]\n","Train Epoch: 149/200 [78400/79999 (100%)]  Loss: 12.101894: 100%|██████████| 1600/1600 [00:06<00:00, 240.07it/s]\n","Train Epoch: 150/200 [78400/79999 (100%)]  Loss: 11.915131: 100%|██████████| 1600/1600 [00:06<00:00, 242.46it/s]\n","Train Epoch: 151/200 [78400/79999 (100%)]  Loss: 11.701087: 100%|██████████| 1600/1600 [00:06<00:00, 243.18it/s]\n","Train Epoch: 152/200 [78400/79999 (100%)]  Loss: 12.107920: 100%|██████████| 1600/1600 [00:06<00:00, 245.44it/s]\n","Train Epoch: 153/200 [78400/79999 (100%)]  Loss: 12.363225: 100%|██████████| 1600/1600 [00:06<00:00, 240.84it/s]\n","Train Epoch: 154/200 [78400/79999 (100%)]  Loss: 11.615650: 100%|██████████| 1600/1600 [00:06<00:00, 241.72it/s]\n","Train Epoch: 155/200 [78400/79999 (100%)]  Loss: 11.975916: 100%|██████████| 1600/1600 [00:06<00:00, 242.82it/s]\n","Train Epoch: 156/200 [78400/79999 (100%)]  Loss: 12.462059: 100%|██████████| 1600/1600 [00:06<00:00, 245.99it/s]\n","Train Epoch: 157/200 [78400/79999 (100%)]  Loss: 12.054519: 100%|██████████| 1600/1600 [00:06<00:00, 245.13it/s]\n","Train Epoch: 158/200 [78400/79999 (100%)]  Loss: 11.305238: 100%|██████████| 1600/1600 [00:06<00:00, 238.78it/s]\n","Train Epoch: 159/200 [78400/79999 (100%)]  Loss: 11.565286: 100%|██████████| 1600/1600 [00:06<00:00, 241.28it/s]\n","Train Epoch: 160/200 [78400/79999 (100%)]  Loss: 11.785523: 100%|██████████| 1600/1600 [00:06<00:00, 244.90it/s]\n","Train Epoch: 161/200 [78400/79999 (100%)]  Loss: 11.880108: 100%|██████████| 1600/1600 [00:06<00:00, 244.49it/s]\n","Train Epoch: 162/200 [78400/79999 (100%)]  Loss: 11.472708: 100%|██████████| 1600/1600 [00:06<00:00, 241.55it/s]\n","Train Epoch: 163/200 [78400/79999 (100%)]  Loss: 11.957126: 100%|██████████| 1600/1600 [00:06<00:00, 242.39it/s]\n","Train Epoch: 164/200 [78400/79999 (100%)]  Loss: 11.402408: 100%|██████████| 1600/1600 [00:06<00:00, 242.61it/s]\n","Train Epoch: 165/200 [78400/79999 (100%)]  Loss: 11.447035: 100%|██████████| 1600/1600 [00:06<00:00, 245.31it/s]\n","Train Epoch: 166/200 [78400/79999 (100%)]  Loss: 11.098412: 100%|██████████| 1600/1600 [00:06<00:00, 243.51it/s]\n","Train Epoch: 167/200 [78400/79999 (100%)]  Loss: 11.727457: 100%|██████████| 1600/1600 [00:06<00:00, 244.04it/s]\n","Train Epoch: 168/200 [78400/79999 (100%)]  Loss: 10.691822: 100%|██████████| 1600/1600 [00:06<00:00, 241.80it/s]\n","Train Epoch: 169/200 [78400/79999 (100%)]  Loss: 11.409717: 100%|██████████| 1600/1600 [00:06<00:00, 243.56it/s]\n","Train Epoch: 170/200 [78400/79999 (100%)]  Loss: 11.483028: 100%|██████████| 1600/1600 [00:06<00:00, 240.66it/s]\n","Train Epoch: 171/200 [78400/79999 (100%)]  Loss: 11.563768: 100%|██████████| 1600/1600 [00:06<00:00, 240.74it/s]\n","Train Epoch: 172/200 [78400/79999 (100%)]  Loss: 11.211480: 100%|██████████| 1600/1600 [00:06<00:00, 244.21it/s]\n","Train Epoch: 173/200 [78400/79999 (100%)]  Loss: 11.156301: 100%|██████████| 1600/1600 [00:06<00:00, 244.87it/s]\n","Train Epoch: 174/200 [78400/79999 (100%)]  Loss: 11.334943: 100%|██████████| 1600/1600 [00:06<00:00, 245.75it/s]\n","Train Epoch: 175/200 [78400/79999 (100%)]  Loss: 10.772857: 100%|██████████| 1600/1600 [00:06<00:00, 245.99it/s]\n","Train Epoch: 176/200 [78400/79999 (100%)]  Loss: 11.050335: 100%|██████████| 1600/1600 [00:06<00:00, 244.57it/s]\n","Train Epoch: 177/200 [78400/79999 (100%)]  Loss: 11.247677: 100%|██████████| 1600/1600 [00:06<00:00, 242.38it/s]\n","Train Epoch: 178/200 [78400/79999 (100%)]  Loss: 10.526941: 100%|██████████| 1600/1600 [00:07<00:00, 217.75it/s]\n","Train Epoch: 179/200 [78400/79999 (100%)]  Loss: 10.899230: 100%|██████████| 1600/1600 [00:06<00:00, 245.27it/s]\n","Train Epoch: 180/200 [78400/79999 (100%)]  Loss: 10.951053: 100%|██████████| 1600/1600 [00:06<00:00, 243.24it/s]\n","Train Epoch: 181/200 [78400/79999 (100%)]  Loss: 11.365563: 100%|██████████| 1600/1600 [00:06<00:00, 242.58it/s]\n","Train Epoch: 182/200 [78400/79999 (100%)]  Loss: 10.977485: 100%|██████████| 1600/1600 [00:06<00:00, 244.71it/s]\n","Train Epoch: 183/200 [78400/79999 (100%)]  Loss: 11.334085: 100%|██████████| 1600/1600 [00:06<00:00, 240.93it/s]\n","Train Epoch: 184/200 [78400/79999 (100%)]  Loss: 10.602744: 100%|██████████| 1600/1600 [00:06<00:00, 245.09it/s]\n","Train Epoch: 185/200 [78400/79999 (100%)]  Loss: 10.215415: 100%|██████████| 1600/1600 [00:06<00:00, 245.75it/s]\n","Train Epoch: 186/200 [78400/79999 (100%)]  Loss: 10.418052: 100%|██████████| 1600/1600 [00:06<00:00, 241.87it/s]\n","Train Epoch: 187/200 [78400/79999 (100%)]  Loss: 10.390591: 100%|██████████| 1600/1600 [00:06<00:00, 244.30it/s]\n","Train Epoch: 188/200 [78400/79999 (100%)]  Loss: 10.596122: 100%|██████████| 1600/1600 [00:06<00:00, 244.02it/s]\n","Train Epoch: 189/200 [78400/79999 (100%)]  Loss: 10.580855: 100%|██████████| 1600/1600 [00:06<00:00, 244.39it/s]\n","Train Epoch: 190/200 [78400/79999 (100%)]  Loss: 10.743867: 100%|██████████| 1600/1600 [00:06<00:00, 244.25it/s]\n","Train Epoch: 191/200 [78400/79999 (100%)]  Loss: 10.609648: 100%|██████████| 1600/1600 [00:06<00:00, 246.15it/s]\n","Train Epoch: 192/200 [78400/79999 (100%)]  Loss: 10.612889: 100%|██████████| 1600/1600 [00:06<00:00, 244.72it/s]\n","Train Epoch: 193/200 [78400/79999 (100%)]  Loss: 10.449427: 100%|██████████| 1600/1600 [00:06<00:00, 241.45it/s]\n","Train Epoch: 194/200 [78400/79999 (100%)]  Loss: 10.405861: 100%|██████████| 1600/1600 [00:06<00:00, 243.46it/s]\n","Train Epoch: 195/200 [78400/79999 (100%)]  Loss: 10.388822: 100%|██████████| 1600/1600 [00:06<00:00, 241.39it/s]\n","Train Epoch: 196/200 [78400/79999 (100%)]  Loss: 10.184201: 100%|██████████| 1600/1600 [00:06<00:00, 241.94it/s]\n","Train Epoch: 197/200 [78400/79999 (100%)]  Loss: 10.509972: 100%|██████████| 1600/1600 [00:06<00:00, 241.93it/s]\n","Train Epoch: 198/200 [78400/79999 (100%)]  Loss: 9.694865: 100%|██████████| 1600/1600 [00:06<00:00, 241.28it/s]\n","Train Epoch: 199/200 [78400/79999 (100%)]  Loss: 9.727492: 100%|██████████| 1600/1600 [00:06<00:00, 244.35it/s]\n","Train Epoch: 200/200 [78400/79999 (100%)]  Loss: 10.530724: 100%|██████████| 1600/1600 [00:06<00:00, 240.45it/s]\n"]}],"source":["\n","#Teacher = load_model(Teacher, save_path)\n","Teacher = Teachertrain(Teacher, train_loader, learning_rate = 0.0003, num_epochs = 200)\n"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2245,"status":"ok","timestamp":1668050086035,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"rCeFE-vKJbb1","outputId":"4f7f7580-7240-4e04-e5ec-f74224bf9be0"},"outputs":[{"output_type":"stream","name":"stdout","text":["false positive (FP): 141, false negative (FN): 372, true positive (TP): 134, true negative (TN): 19352\n","Test set: Average loss: 0.0029, Accuracy: 97.43%). Total time = 1.7298221588134766\n","Precision: 48.727%, Recall: 26.482%, F1-measure: 34.315%\n"]}],"source":["Teacher = load_model(Teacher, save_path)\n","start_time = time()\n","accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(Teacher, criterion = nn.CrossEntropyLoss())\n","test_loss /= (split*sub)\n","\n","print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n","print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%). Total time = {time() - start_time}')\n","print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'\n","            .format(P, R, F1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dihRKTMBFO3E","outputId":"b95fe792-51a8-4135-c43c-d98a5dcb1189"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 1600/1600 [00:06<00:00, 257.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.82\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 1600/1600 [00:06<00:00, 266.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.87\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 1600/1600 [00:06<00:00, 266.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.89\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 1600/1600 [00:06<00:00, 262.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loss:-0.90\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5:  56%|█████▋    | 903/1600 [00:03<00:02, 263.44it/s]"]}],"source":["def train_step(\n","    Teacher,\n","    Student,\n","    optimizer,\n","    student_loss_fn,\n","    divergence_loss_fn,\n","    temp,\n","    alpha,\n","    epoch,\n","    device\n","):\n","    losses = []\n","    pbar = tqdm(train_loader, total=len(train_loader), position=0, leave=True, desc=f\"Epoch {epoch+1}\")\n","    for data, targets in pbar:\n","        # Get data to cuda if possible\n","        data = data.to(device)\n","        targets = targets.to(device)\n","\n","        # forward\n","        with torch.no_grad():\n","            teacher_preds = Teacher(data)\n","\n","        student_preds = Student(data)\n","        student_loss = student_loss_fn(student_preds, targets)\n","        \n","        distillation_loss = divergence_loss_fn(\n","            F.softmax(student_preds / temp, dim=1),\n","            F.softmax(teacher_preds / temp, dim=1)\n","        )\n","        loss = alpha * student_loss + (1 - alpha) * distillation_loss\n","        #print(student_loss)\n","        #print(distillation_loss)\n","        losses.append(loss.item())\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","\n","        optimizer.step()\n","    \n","    avg_loss = sum(losses) / len(losses)\n","    return avg_loss\n","\n","def teach(epochs, Teacher, Student, temp=7, alpha=0.3):\n","  Teacher = Teacher.to(device)\n","  Student = Student.to(device)\n","  student_loss_fn = nn.CrossEntropyLoss()\n","  divergence_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n","  optimizer = torch.optim.Adam(Student.parameters(), lr=0.01)\n","\n","  Teacher.eval()\n","  Student.train()\n","  for epoch in range(epochs):\n","      loss = train_step(\n","          Teacher,\n","          Student,\n","          optimizer,\n","          student_loss_fn,\n","          divergence_loss_fn,\n","          temp,\n","          alpha,\n","          epoch,\n","          device\n","      )\n","\n","      print(f\"Loss:{loss:.2f}\")\n","      \n","\n","teach(epochs=100, Teacher=Teacher, Student=Student, temp=7, alpha=0.3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybaPG5FeVG2V"},"outputs":[],"source":["start_time = time()\n","accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(Student, criterion = nn.CrossEntropyLoss())\n","test_loss /= (split*sub)\n","\n","print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n","print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%). Total time = {time() - start_time}')\n","print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'\n","            .format(P, R, F1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"weJj_DJjVeL0"},"outputs":[],"source":["noKD = StudentModel(input_size = 30, hidden_size = 6, num_layers = 2, num_classes = 2).to(device) \n","noKD = Teachertrain(noKD, train_loader, learning_rate, num_epochs = 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wh_-odz2V2G7"},"outputs":[],"source":["start_time = time()\n","accuracy, test_loss, P, R, F1, TP, FP, TN, FN = test(noKD, criterion = nn.CrossEntropyLoss())\n","test_loss /= (split*sub)\n","\n","print('false positive (FP): {}, false negative (FN): {}, true positive (TP): {}, true negative (TN): {}'.format(FP, FN, TP, TN))\n","print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%). Total time = {time() - start_time}')\n","print('Precision: {:.3f}%, Recall: {:.3f}%, F1-measure: {:.3f}%'\n","            .format(P, R, F1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UI3i8mURX6SW"},"outputs":[],"source":["save_model(Teacher,'./datasets/Spirit/model/no-attention-teacher.pth')\n","save_model(Student,'./datasets/Spirit/model/no-attention-student.pth')\n","save_model(noKD,'./datasets/Spirit/model/no-attention-noKD.pth')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}