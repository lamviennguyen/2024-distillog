{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":352,"status":"ok","timestamp":1740634715614,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"lVivHDd5CvYt","outputId":"3847ad32-ca8a-465d-f25f-6391cf330d5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab/distillog/hdfs-kd-unsupervised\n"]}],"source":["%cd /content/drive/MyDrive/colab/distillog/hdfs-kd-unsupervised"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4894,"status":"ok","timestamp":1740634720510,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"eMZwCTLQEneZ","outputId":"b2a7827b-28a9-4081-d6ad-c2e1e00affaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting overrides\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Installing collected packages: overrides\n","Successfully installed overrides-7.7.0\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}],"source":["!pip install overrides\n","!pip install torchinfo"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4321299,"status":"ok","timestamp":1740639041811,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"},"user_tz":-420},"id":"qXj1B0BjJ-1m","outputId":"56cc98e6-0963-41fe-c1fd-62680c22aaf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","DistilLog                                [50, 50, 30]              --\n","├─GRU: 1-1                               [50, 50, 32]              12,480\n","├─Linear: 1-2                            [50, 50, 8]               264\n","├─Linear: 1-3                            [50, 50, 1]               9\n","├─Linear: 1-4                            [50, 50, 8]               16\n","├─Linear: 1-5                            [50, 50, 30]              270\n","├─MSELoss: 1-6                           [50, 50, 30]              --\n","==========================================================================================\n","Total params: 13,039\n","Trainable params: 13,039\n","Non-trainable params: 0\n","Total mult-adds (Units.MEGABYTES): 31.23\n","==========================================================================================\n","Input size (MB): 0.30\n","Forward/backward pass size (MB): 1.58\n","Params size (MB): 0.05\n","Estimated Total Size (MB): 1.93\n","==========================================================================================\n","epoch 1 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.50it/s]\n","total loss: 20.022118853696156\n","epoch 2 / 50 :\n","100% 4466/4466 [01:28<00:00, 50.32it/s]\n","total loss: 5.655483695620205\n","epoch 3 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.63it/s]\n","total loss: 6.130223308282439\n","epoch 4 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.45it/s]\n","total loss: 5.587812012177892\n","epoch 5 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.41it/s]\n","total loss: 5.71787318441784\n","epoch 6 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.49it/s]\n","total loss: 5.6025872469181195\n","epoch 7 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.30it/s]\n","total loss: 5.714583969907835\n","epoch 8 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.56it/s]\n","total loss: 5.602820037922356\n","epoch 9 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.31it/s]\n","total loss: 6.333917049982119\n","epoch 10 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.71it/s]\n","total loss: 5.570115830167197\n","epoch 11 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.42it/s]\n","total loss: 5.649092698469758\n","epoch 12 / 50 :\n","100% 4466/4466 [01:29<00:00, 49.76it/s]\n","total loss: 6.52911548037082\n","epoch 13 / 50 :\n","100% 4466/4466 [01:25<00:00, 51.97it/s]\n","total loss: 5.54833580146078\n","epoch 14 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.54it/s]\n","total loss: 5.5697861894150265\n","epoch 15 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.56it/s]\n","total loss: 6.195821578905452\n","epoch 16 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.60it/s]\n","total loss: 5.8160051495651715\n","epoch 17 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.78it/s]\n","total loss: 5.56476716324687\n","epoch 18 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.77it/s]\n","total loss: 5.912627934361808\n","epoch 19 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.67it/s]\n","total loss: 5.704521546198521\n","epoch 20 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.58it/s]\n","total loss: 5.5368319294648245\n","epoch 21 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.46it/s]\n","total loss: 5.547011298127472\n","epoch 22 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.57it/s]\n","total loss: 5.562691777013242\n","epoch 23 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.45it/s]\n","total loss: 5.527197697607335\n","epoch 24 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.65it/s]\n","total loss: 5.554755056858994\n","epoch 25 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.68it/s]\n","total loss: 5.583652186614927\n","epoch 26 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.60it/s]\n","total loss: 5.864808354759589\n","epoch 27 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.40it/s]\n","total loss: 5.636603150400333\n","epoch 28 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.51it/s]\n","total loss: 5.812371210078709\n","epoch 29 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.67it/s]\n","total loss: 5.5555250305333175\n","epoch 30 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.56it/s]\n","total loss: 5.531872340478003\n","epoch 31 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.39it/s]\n","total loss: 5.560049929132219\n","epoch 32 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.44it/s]\n","total loss: 5.686369556176942\n","epoch 33 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.47it/s]\n","total loss: 5.523385276028421\n","epoch 34 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.06it/s]\n","total loss: 5.596152305311989\n","epoch 35 / 50 :\n","100% 4466/4466 [01:27<00:00, 50.89it/s]\n","total loss: 5.541293599118944\n","epoch 36 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.18it/s]\n","total loss: 5.652437123644631\n","epoch 37 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.49it/s]\n","total loss: 5.560365191835444\n","epoch 38 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.11it/s]\n","total loss: 5.584496118535753\n","epoch 39 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.04it/s]\n","total loss: 5.543273070594296\n","epoch 40 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.13it/s]\n","total loss: 5.596579619450495\n","epoch 41 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.52it/s]\n","total loss: 5.61098612670321\n","epoch 42 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.51it/s]\n","total loss: 5.480074654391501\n","epoch 43 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.74it/s]\n","total loss: 5.7131318565807305\n","epoch 44 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.38it/s]\n","total loss: 5.482338403526228\n","epoch 45 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.55it/s]\n","total loss: 5.670492913399357\n","epoch 46 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.07it/s]\n","total loss: 5.542545352189336\n","epoch 47 / 50 :\n","100% 4466/4466 [01:24<00:00, 52.57it/s]\n","total loss: 5.554011617321521\n","epoch 48 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.46it/s]\n","total loss: 5.481393166643102\n","epoch 49 / 50 :\n","100% 4466/4466 [01:26<00:00, 51.47it/s]\n","total loss: 5.704027312749531\n","epoch 50 / 50 :\n","100% 4466/4466 [01:25<00:00, 52.50it/s]\n","total loss: 5.524015661503654\n"]}],"source":["!python train.py"]},{"cell_type":"code","source":["!python test.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-jGM_KScB8o","executionInfo":{"status":"ok","timestamp":1740639279374,"user_tz":-420,"elapsed":237545,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"c7df10e4-0a8c-4a64-d3e3-c6c166bce340"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","/content/drive/MyDrive/colab/distillog/hdfs-kd-unsupervised/utils.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(save_path, map_location = torch.device('cpu')))\n","100% 1286/1286 [00:27<00:00, 46.79it/s]\n","1 [TP, TN, FP, FN] = 16332 4323 107375 505\n","thre: 1.000\n","Accuracy: 16.070%, Precision: 13.202%, Recall: 97.001%, F1-measure: 23.241%\n","100% 1286/1286 [00:25<00:00, 49.56it/s]\n","2 [TP, TN, FP, FN] = 15824 16088 95610 1013\n","thre: 2.000\n","Accuracy: 24.827%, Precision: 14.200%, Recall: 93.983%, F1-measure: 24.673%\n","100% 1286/1286 [00:20<00:00, 62.01it/s]\n","3 [TP, TN, FP, FN] = 15113 36633 75065 1724\n","thre: 3.000\n","Accuracy: 40.258%, Precision: 16.759%, Recall: 89.761%, F1-measure: 28.245%\n","100% 1286/1286 [00:22<00:00, 56.99it/s]\n","4 [TP, TN, FP, FN] = 14088 60268 51430 2749\n","thre: 4.000\n","Accuracy: 57.849%, Precision: 21.502%, Recall: 83.673%, F1-measure: 34.213%\n","100% 1286/1286 [00:24<00:00, 53.28it/s]\n","5 [TP, TN, FP, FN] = 13250 82901 28797 3587\n","thre: 5.000\n","Accuracy: 74.805%, Precision: 31.512%, Recall: 78.696%, F1-measure: 45.004%\n","100% 1286/1286 [00:20<00:00, 61.53it/s]\n","6 [TP, TN, FP, FN] = 11768 97324 14374 5069\n","thre: 6.000\n","Accuracy: 84.873%, Precision: 45.016%, Recall: 69.894%, F1-measure: 54.762%\n","100% 1286/1286 [00:22<00:00, 56.82it/s]\n","7 [TP, TN, FP, FN] = 10640 104771 6927 6197\n","thre: 7.000\n","Accuracy: 89.790%, Precision: 60.568%, Recall: 63.194%, F1-measure: 61.853%\n","100% 1286/1286 [00:21<00:00, 59.40it/s]\n","8 [TP, TN, FP, FN] = 8879 107938 3760 7958\n","thre: 8.000\n","Accuracy: 90.883%, Precision: 70.251%, Recall: 52.735%, F1-measure: 60.246%\n","100% 1286/1286 [00:21<00:00, 58.72it/s]\n","9 [TP, TN, FP, FN] = 7905 110358 1340 8932\n","thre: 9.000\n","Accuracy: 92.008%, Precision: 85.506%, Recall: 46.950%, F1-measure: 60.617%\n","100% 1286/1286 [00:22<00:00, 56.51it/s]\n","10 [TP, TN, FP, FN] = 6457 111250 448 10380\n","thre: 10.000\n","Accuracy: 91.576%, Precision: 93.512%, Recall: 38.350%, F1-measure: 54.393%\n","7 61.853272875247065\n"]}]},{"cell_type":"code","source":["!python teach.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n862ixlzNKko","executionInfo":{"status":"ok","timestamp":1740640221369,"user_tz":-420,"elapsed":602241,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"f84c6b9a-6c2a-4e49-a0e9-ee173b87863a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab/distillog/hdfs-kd-unsupervised/utils.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(save_path, map_location = torch.device('cpu')))\n","100% 20/20 [02:03<00:00,  6.18s/it]\n","Epoch 1 -  Loss:-2.75\n","100% 20/20 [01:58<00:00,  5.94s/it]\n","Epoch 2 -  Loss:-2.75\n","100% 20/20 [01:55<00:00,  5.79s/it]\n","Epoch 3 -  Loss:-2.75\n","100% 20/20 [01:58<00:00,  5.91s/it]\n","Epoch 4 -  Loss:-2.75\n","100% 20/20 [01:57<00:00,  5.87s/it]\n","Epoch 5 -  Loss:-2.75\n"]}]},{"cell_type":"code","source":["!python test.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BClP5X48Nh6o","executionInfo":{"status":"ok","timestamp":1740640454925,"user_tz":-420,"elapsed":233167,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"1e94a2ff-c650-4bc8-8fd3-f0610ff0099c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","/content/drive/MyDrive/colab/distillog/hdfs-kd-unsupervised/utils.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(save_path, map_location = torch.device('cpu')))\n","100% 1286/1286 [00:23<00:00, 54.73it/s]\n","1 [TP, TN, FP, FN] = 14178 11758 99940 2659\n","thre: 6.000\n","Accuracy: 20.178%, Precision: 12.424%, Recall: 84.207%, F1-measure: 21.653%\n","100% 1286/1286 [00:22<00:00, 56.46it/s]\n","2 [TP, TN, FP, FN] = 14098 14603 97095 2739\n","thre: 7.000\n","Accuracy: 22.329%, Precision: 12.679%, Recall: 83.732%, F1-measure: 22.023%\n","100% 1286/1286 [00:22<00:00, 56.74it/s]\n","3 [TP, TN, FP, FN] = 13955 17031 94667 2882\n","thre: 8.000\n","Accuracy: 24.107%, Precision: 12.847%, Recall: 82.883%, F1-measure: 22.246%\n","100% 1286/1286 [00:23<00:00, 53.97it/s]\n","4 [TP, TN, FP, FN] = 13816 18913 92785 3021\n","thre: 9.000\n","Accuracy: 25.463%, Precision: 12.960%, Recall: 82.057%, F1-measure: 22.385%\n","100% 1286/1286 [00:22<00:00, 56.81it/s]\n","5 [TP, TN, FP, FN] = 13620 20078 91620 3217\n","thre: 10.000\n","Accuracy: 26.217%, Precision: 12.942%, Recall: 80.893%, F1-measure: 22.314%\n","100% 1286/1286 [00:22<00:00, 56.63it/s]\n","6 [TP, TN, FP, FN] = 13455 20743 90955 3382\n","thre: 11.000\n","Accuracy: 26.606%, Precision: 12.887%, Recall: 79.913%, F1-measure: 22.194%\n","100% 1286/1286 [00:23<00:00, 55.06it/s]\n","7 [TP, TN, FP, FN] = 13274 22053 89645 3563\n","thre: 12.000\n","Accuracy: 27.484%, Precision: 12.898%, Recall: 78.838%, F1-measure: 22.168%\n","100% 1286/1286 [00:21<00:00, 58.96it/s]\n","8 [TP, TN, FP, FN] = 13050 23717 87981 3787\n","thre: 13.000\n","Accuracy: 28.605%, Precision: 12.917%, Recall: 77.508%, F1-measure: 22.143%\n","100% 1286/1286 [00:23<00:00, 55.44it/s]\n","9 [TP, TN, FP, FN] = 12858 25688 86010 3979\n","thre: 14.000\n","Accuracy: 29.989%, Precision: 13.005%, Recall: 76.368%, F1-measure: 22.225%\n","100% 1286/1286 [00:23<00:00, 54.72it/s]\n","10 [TP, TN, FP, FN] = 12754 27816 83882 4083\n","thre: 15.000\n","Accuracy: 31.563%, Precision: 13.198%, Recall: 75.750%, F1-measure: 22.479%\n","15 22.479356322649444\n"]}]},{"cell_type":"code","source":["!python test.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DnWYds5sQh2P","executionInfo":{"status":"ok","timestamp":1740640948536,"user_tz":-420,"elapsed":451252,"user":{"displayName":"Lâm Viên Nguyễn","userId":"08300517389315092319"}},"outputId":"f843989e-0239-474e-d31f-8813fa02f6cd"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","/content/drive/MyDrive/colab/distillog/hdfs-kd-unsupervised/utils.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(save_path, map_location = torch.device('cpu')))\n","100% 1286/1286 [00:23<00:00, 55.83it/s]\n","1 [TP, TN, FP, FN] = 12579 32253 79445 4258\n","thre: 16.000\n","Accuracy: 34.879%, Precision: 13.669%, Recall: 74.710%, F1-measure: 23.110%\n","100% 1286/1286 [00:23<00:00, 55.07it/s]\n","2 [TP, TN, FP, FN] = 12426 37068 74630 4411\n","thre: 17.000\n","Accuracy: 38.506%, Precision: 14.274%, Recall: 73.802%, F1-measure: 23.921%\n","100% 1286/1286 [00:21<00:00, 60.47it/s]\n","3 [TP, TN, FP, FN] = 12285 42454 69244 4552\n","thre: 18.000\n","Accuracy: 42.587%, Precision: 15.068%, Recall: 72.964%, F1-measure: 24.978%\n","100% 1286/1286 [00:22<00:00, 56.16it/s]\n","4 [TP, TN, FP, FN] = 12151 48451 63247 4686\n","thre: 19.000\n","Accuracy: 47.148%, Precision: 16.116%, Recall: 72.168%, F1-measure: 26.348%\n","100% 1286/1286 [00:22<00:00, 56.67it/s]\n","5 [TP, TN, FP, FN] = 11993 55651 56047 4844\n","thre: 20.000\n","Accuracy: 52.627%, Precision: 17.626%, Recall: 71.230%, F1-measure: 28.260%\n","100% 1286/1286 [00:21<00:00, 60.42it/s]\n","6 [TP, TN, FP, FN] = 11849 61255 50443 4988\n","thre: 21.000\n","Accuracy: 56.875%, Precision: 19.022%, Recall: 70.375%, F1-measure: 29.949%\n","100% 1286/1286 [00:22<00:00, 56.56it/s]\n","7 [TP, TN, FP, FN] = 11725 66575 45123 5112\n","thre: 22.000\n","Accuracy: 60.917%, Precision: 20.625%, Recall: 69.638%, F1-measure: 31.825%\n","100% 1286/1286 [00:21<00:00, 58.92it/s]\n","8 [TP, TN, FP, FN] = 11543 70386 41312 5294\n","thre: 23.000\n","Accuracy: 63.741%, Precision: 21.839%, Recall: 68.557%, F1-measure: 33.126%\n","100% 1286/1286 [00:22<00:00, 57.27it/s]\n","9 [TP, TN, FP, FN] = 11411 73276 38422 5426\n","thre: 24.000\n","Accuracy: 65.886%, Precision: 22.898%, Recall: 67.773%, F1-measure: 34.231%\n","100% 1286/1286 [00:22<00:00, 56.36it/s]\n","10 [TP, TN, FP, FN] = 11312 75556 36142 5525\n","thre: 25.000\n","Accuracy: 67.583%, Precision: 23.838%, Recall: 67.185%, F1-measure: 35.190%\n","100% 1286/1286 [00:21<00:00, 61.21it/s]\n","11 [TP, TN, FP, FN] = 11242 77207 34491 5595\n","thre: 26.000\n","Accuracy: 68.813%, Precision: 24.582%, Recall: 66.770%, F1-measure: 35.934%\n","100% 1286/1286 [00:22<00:00, 56.10it/s]\n","12 [TP, TN, FP, FN] = 11149 78420 33278 5688\n","thre: 27.000\n","Accuracy: 69.685%, Precision: 25.095%, Recall: 66.217%, F1-measure: 36.397%\n","100% 1286/1286 [00:23<00:00, 55.51it/s]\n","13 [TP, TN, FP, FN] = 11024 79683 32015 5813\n","thre: 28.000\n","Accuracy: 70.570%, Precision: 25.614%, Recall: 65.475%, F1-measure: 36.823%\n","100% 1286/1286 [00:21<00:00, 60.58it/s]\n","14 [TP, TN, FP, FN] = 10907 80096 31602 5930\n","thre: 29.000\n","Accuracy: 70.800%, Precision: 25.658%, Recall: 64.780%, F1-measure: 36.757%\n","100% 1286/1286 [00:22<00:00, 56.32it/s]\n","15 [TP, TN, FP, FN] = 10785 80280 31418 6052\n","thre: 30.000\n","Accuracy: 70.848%, Precision: 25.555%, Recall: 64.055%, F1-measure: 36.535%\n","100% 1286/1286 [00:22<00:00, 57.70it/s]\n","16 [TP, TN, FP, FN] = 10671 80280 31418 6166\n","thre: 31.000\n","Accuracy: 70.760%, Precision: 25.353%, Recall: 63.378%, F1-measure: 36.218%\n","100% 1286/1286 [00:22<00:00, 58.44it/s]\n","17 [TP, TN, FP, FN] = 10596 80327 31371 6241\n","thre: 32.000\n","Accuracy: 70.738%, Precision: 25.248%, Recall: 62.933%, F1-measure: 36.038%\n","100% 1286/1286 [00:22<00:00, 55.97it/s]\n","18 [TP, TN, FP, FN] = 10489 80374 31324 6348\n","thre: 33.000\n","Accuracy: 70.691%, Precision: 25.085%, Recall: 62.297%, F1-measure: 35.768%\n","100% 1286/1286 [00:21<00:00, 59.66it/s]\n","19 [TP, TN, FP, FN] = 10306 80374 31324 6531\n","thre: 34.000\n","Accuracy: 70.549%, Precision: 24.756%, Recall: 61.210%, F1-measure: 35.254%\n","100% 1286/1286 [00:23<00:00, 55.04it/s]\n","20 [TP, TN, FP, FN] = 10156 80374 31324 6681\n","thre: 35.000\n","Accuracy: 70.432%, Precision: 24.484%, Recall: 60.320%, F1-measure: 34.830%\n","28 36.82276705190727\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1REwSdizxgwsj4m4TylWwig4Cw440J8CW","authorship_tag":"ABX9TyPLXx4KGrf2h9ud21TEWdYZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}